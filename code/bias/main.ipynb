{"nbformat":4,"nbformat_minor":5,"metadata":{"colab":{"name":"main.ipynb","provenance":[{"file_id":"1pCgno1Q4Nj1-IsW0uvrkN2aBLCF-ojC3","timestamp":1638686991905},{"file_id":"1ai5uHH2Wy2cwZ85dbJUsakconxB1tvta","timestamp":1638589251474}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"fd7baa5d4de94e1da0e246ca95aa5f24":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_eeae52394a16483d9960553347906f38","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_0f045c426fa748d78fe8171781756a7c","IPY_MODEL_19f2a44abc1248828277d55c2e648097","IPY_MODEL_63e4b06bc7244857baf4593562f63847"]}},"eeae52394a16483d9960553347906f38":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0f045c426fa748d78fe8171781756a7c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_fc518c5b70724ce9b8c2639ce3bfe858","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3d53c1d1114e42ed99f8e08ae63b3249"}},"19f2a44abc1248828277d55c2e648097":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_80a19ec770da4646a1110276ea470da9","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":570,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":570,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_78a8631585594898ad1a247b0d8e71ac"}},"63e4b06bc7244857baf4593562f63847":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_bb106d3930bc4b24b461092f2c3072e6","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 570/570 [00:00&lt;00:00, 24.0kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_78c6c0841f1c49c7b0cbd409672fb8db"}},"fc518c5b70724ce9b8c2639ce3bfe858":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"3d53c1d1114e42ed99f8e08ae63b3249":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"80a19ec770da4646a1110276ea470da9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"78a8631585594898ad1a247b0d8e71ac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bb106d3930bc4b24b461092f2c3072e6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"78c6c0841f1c49c7b0cbd409672fb8db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8a3ba8cf6f62435082823a0deb1d2205":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_f4a9cb4a730e49b496727bba34543454","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f9d378b9fd6f45b89490a6400bfa8be6","IPY_MODEL_d509c8761fef45c187d7252233b09760","IPY_MODEL_8587edcae5a64f69b6453bceab072def"]}},"f4a9cb4a730e49b496727bba34543454":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f9d378b9fd6f45b89490a6400bfa8be6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_49d9fda89fd74b2caea7b3a521097291","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_579e2cfe2812475892ef90078a4532d8"}},"d509c8761fef45c187d7252233b09760":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_fcf30eabe15a4a62a0577520bab314bf","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":440473133,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":440473133,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_82ebb5412d2141999fc4bbc90354c565"}},"8587edcae5a64f69b6453bceab072def":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ab0cef77bd0d489fbfb274d180cf530a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 420M/420M [00:07&lt;00:00, 61.6MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_16d9c1b732c64373a38ca1983179b66f"}},"49d9fda89fd74b2caea7b3a521097291":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"579e2cfe2812475892ef90078a4532d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fcf30eabe15a4a62a0577520bab314bf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"82ebb5412d2141999fc4bbc90354c565":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ab0cef77bd0d489fbfb274d180cf530a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"16d9c1b732c64373a38ca1983179b66f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e33727377d954afda1704674abd7099c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b952ea221b5f48d4a742c587410fd65d","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_410822e0afb2440a976cb33b4999a0a9","IPY_MODEL_b81d97d9ba89436c9f6d2bf5ea9d8558","IPY_MODEL_17b1ebdbfe8047ba8106606ec9aec2e3"]}},"b952ea221b5f48d4a742c587410fd65d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"410822e0afb2440a976cb33b4999a0a9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e9faa01a89e348b8aab458979c5363d8","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0eb83823ad3c49a0b5e23af64d14649f"}},"b81d97d9ba89436c9f6d2bf5ea9d8558":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_5c2e5ff785334bf99bd40b4821adbcca","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0a872307d263412faf6d1725f4a32803"}},"17b1ebdbfe8047ba8106606ec9aec2e3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4a738bd10a2c479e897036b7f7fd0bc8","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 226k/226k [00:00&lt;00:00, 592kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_bced6864c2394b0bbe3f81b029bd8047"}},"e9faa01a89e348b8aab458979c5363d8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0eb83823ad3c49a0b5e23af64d14649f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5c2e5ff785334bf99bd40b4821adbcca":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"0a872307d263412faf6d1725f4a32803":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4a738bd10a2c479e897036b7f7fd0bc8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"bced6864c2394b0bbe3f81b029bd8047":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b4dd0c3e1ecb4a7cbd607e076d52fe2d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_1258a769bb684938a9834bc1d7a0100a","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_460ca7dbeb874004934f7b38c1c58e85","IPY_MODEL_933b68ae0a834a5ba9c5fdf92ba8222c","IPY_MODEL_aa7a20bc88134d3eb45f162af74f94b4"]}},"1258a769bb684938a9834bc1d7a0100a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"460ca7dbeb874004934f7b38c1c58e85":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0d1e241e3792409baa2e631c2ff7e745","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_75f42cae2e4d47bba25222fc7bbd403b"}},"933b68ae0a834a5ba9c5fdf92ba8222c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_48a64a27c24f4ce490a1aa571e5290cf","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":466062,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":466062,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8cc276d7c7c5479f9e34de2cc4d5f0e1"}},"aa7a20bc88134d3eb45f162af74f94b4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_77fa55c87e3e419eb480950933446a95","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 455k/455k [00:00&lt;00:00, 1.37MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_cf205de3ac0d4110b1ba19af2afada6c"}},"0d1e241e3792409baa2e631c2ff7e745":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"75f42cae2e4d47bba25222fc7bbd403b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"48a64a27c24f4ce490a1aa571e5290cf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"8cc276d7c7c5479f9e34de2cc4d5f0e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"77fa55c87e3e419eb480950933446a95":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"cf205de3ac0d4110b1ba19af2afada6c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"07dfdb754de644eba178dfc95da313cc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_4ab9ac73e2904c27981fdc18f0b83a6c","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_a21ee861c87f4d4fa6ade51740b05199","IPY_MODEL_40584c8639d34ce7952a95911ad5ab1d","IPY_MODEL_b6c9c44f810349349ae03411718a2943"]}},"4ab9ac73e2904c27981fdc18f0b83a6c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a21ee861c87f4d4fa6ade51740b05199":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_cb20a449a79d4146b0eb4d3f5f94ab74","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_eec719933d814600868a07ab8d2b6ef5"}},"40584c8639d34ce7952a95911ad5ab1d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_9a3b3447fd49489cbac9ae61582a8365","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":28,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":28,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1547526e60b846a09ac39333ab1ab53d"}},"b6c9c44f810349349ae03411718a2943":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_845380d4fe0b4d639fefd698337ac6bb","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 28.0/28.0 [00:00&lt;00:00, 1.08kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_59cd15361b9849e0bea53fa94cbae161"}},"cb20a449a79d4146b0eb4d3f5f94ab74":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"eec719933d814600868a07ab8d2b6ef5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9a3b3447fd49489cbac9ae61582a8365":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"1547526e60b846a09ac39333ab1ab53d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"845380d4fe0b4d639fefd698337ac6bb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"59cd15361b9849e0bea53fa94cbae161":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"S4LZFyZ6bB9d","executionInfo":{"status":"ok","timestamp":1638849006319,"user_tz":-480,"elapsed":2486,"user":{"displayName":"Wee Yi Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16336291919603696387"}}},"source":["import tensorflow as tf\n","from tensorflow import keras"],"id":"S4LZFyZ6bB9d","execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"QUBi_CsJeOQe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638849028151,"user_tz":-480,"elapsed":21837,"user":{"displayName":"Wee Yi Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16336291919603696387"}},"outputId":"e00247e9-9e72-4ad5-ad71-bbc82dec57a3"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"id":"QUBi_CsJeOQe","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"JS-Uo5DdnhH9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638849037483,"user_tz":-480,"elapsed":9339,"user":{"displayName":"Wee Yi Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16336291919603696387"}},"outputId":"2d27b0c0-6dfd-4b1c-859e-9ee468ddbf9e"},"source":["!pip install transformers\n","!pip install pytorch-nlp"],"id":"JS-Uo5DdnhH9","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n","\u001b[K     |████████████████████████████████| 3.1 MB 14.9 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 81.0 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 81.7 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n","\u001b[K     |████████████████████████████████| 61 kB 650 kB/s \n","\u001b[?25hCollecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 80.6 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.2.1 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.5\n","Collecting pytorch-nlp\n","  Downloading pytorch_nlp-0.5.0-py3-none-any.whl (90 kB)\n","\u001b[K     |████████████████████████████████| 90 kB 7.3 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-nlp) (1.19.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-nlp) (4.62.3)\n","Installing collected packages: pytorch-nlp\n","Successfully installed pytorch-nlp-0.5.0\n"]}]},{"cell_type":"code","metadata":{"id":"eba4c237","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638849054099,"user_tz":-480,"elapsed":16622,"user":{"displayName":"Wee Yi Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16336291919603696387"}},"outputId":"27209d25-1a73-4b11-f434-db58b25a4742"},"source":["import sys\n","import os\n","\n","os.chdir('/content/drive/MyDrive/NLPProject/bias' )\n","sys.path.append('/content/drive/MyDrive/NLPProject')\n","sys.path.append('/content/drive/MyDrive/NLPProject/bias/')\n","os.environ['OMP_NUM_THREADS'] = \"1\"\n","\n","import argparse\n","import pandas as pd\n","import pickle\n","from model.generator import TransformerDataset, transformer_collate\n","from model.bertmodel import MyBertModel\n","from model.emobertmodel import emoMyBertModel\n","from model.emo_Attention2_bertmodel import emoAtt2MyBertModel\n","from model.lstmmodel import LSTMModel\n","import torch\n","from parameters import BERT_MODEL_PATH, CLAIM_ONLY, CLAIM_AND_EVIDENCE, EVIDENCE_ONLY, DEVICE, INPUT_TYPE_ORDER\n","from transformers import AdamW\n","import numpy as np\n","from utils.utils import print_message, clean_str, preprocess\n","from sklearn.metrics import f1_score\n","from sklearn.utils.class_weight import compute_class_weight\n","from collections import Counter\n","from torchnlp.word_to_vector import GloVe\n","from collections import Counter\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.ensemble import RandomForestClassifier\n","#from hypopt import GridSearch\n","from model_selection import GridSearch\n","from tqdm import tqdm\n","\n","\n","# load the right dataset for different configurations, if preprocessing\n","# is set, it will preprocess the dataset\n","def load_data(dataset, step = 'none'):\n","    #path = \"../../multi_fc_publicdata/\" + dataset + \"/\"\n","\n","    path = \"../multi_fc_publicdata/\" + dataset + \"/\"\n","    print(f'load data function: step = {step}')\n","    if step == 'formal' or step == 'informal':\n","      print('**** load data: formal or informal *****') \n","      main_data = pd.read_csv(path + dataset + \"_\" + step + \".tsv\", sep=\"\\t\", header=None)\n","      snippets_data = pd.read_csv(path + dataset + \"_\" + step + \"_snippets.tsv\", sep=\"\\t\", header=None)    \n","    elif step == 'none' or step == 'EMO_LEXI' or step == 'EMO_INT' or step == 'EMO_ATT2_LEXI' or step == 'EMO_ATT2_INT':\n","      print('****load data: none-preprocess, EMO_LEXI, EMO_INT, EMO_ATT2_LEXI, EMO_ATT2_INT *****') \n","      main_data = pd.read_csv(path + dataset + \".tsv\", sep=\"\\t\", header=None)\n","      snippets_data = pd.read_csv(path + dataset + \"_snippets.tsv\", sep=\"\\t\", header=None)    \n","    elif step == 'neutralized': \n","      print('****load data: neutralized *****') \n","      main_data = pd.read_csv(path + dataset + \"_\" + step + \".tsv\", sep=\"\\t\", header=None)     \n","      snippets_data = pd.read_csv(path + dataset + \"_snippets.tsv\", sep=\"\\t\", header=None)    \n","    else: \n","      print('****load data: preprocess *****') \n","      main_data = pd.read_csv(path + dataset + \".tsv\", sep=\"\\t\", header=None)\n","      snippets_data = pd.read_csv(path + dataset + \"_snippets.tsv\", sep=\"\\t\", header=None)\n","\n","      for index, row in main_data.iterrows():\n","        main_data[1][index] = preprocess(row[1], step)     \n","      \n","          \n","      for index, row in snippets_data.iterrows():\n","        snippets_data[1][index] = preprocess(row[1], step)\n","        snippets_data[2][index] = preprocess(row[2], step)\n","        snippets_data[3][index] = preprocess(row[3], step)\n","        snippets_data[4][index] = preprocess(row[4], step)\n","        snippets_data[5][index] = preprocess(row[5], step)\n","        snippets_data[6][index] = preprocess(row[6], step)\n","        snippets_data[7][index] = preprocess(row[7], step)\n","        snippets_data[8][index] = preprocess(row[8], step)\n","        snippets_data[9][index] = preprocess(row[9], step)\n","        snippets_data[10][index] = preprocess(row[10], step)\n","    \n","    label_order = pickle.load(open(path + dataset + \"_labels.pkl\", \"rb\"))\n","    splits = pickle.load(open(path + dataset + \"_index_split.pkl\", \"rb\"))\n","\n","    return main_data, snippets_data, label_order, splits\n","\n","# Generating dataset loader for the claims and snippet data.\n","def make_generators(main_data, snippets_data, label_order, splits, params, dataset_generator=TransformerDataset, other_dataset=False):\n","    generators = []\n","\n","    all_labels = main_data.values[:,2]\n","    counter = Counter(all_labels)\n","    ss = \"\"\n","    for c in label_order:\n","        ss = ss + \", \" + str(c) + \" (\" + str(np.around(counter[c]/len(all_labels) * 100,1)) + \"\\%)\"\n","        #print(c, np.around(counter[c]/len(all_labels) * 100,1), \"%\", counter[c])\n","    print(\"len\", len(all_labels), ss)\n","\n","    for isplit, split in enumerate(splits):\n","        # print(f'isplit {isplit}')\n","        sub_main_data = main_data.values[split]\n","        # print(f'len sub_main_data: {len(sub_main_data)}')\n","        \n","        sub_snippets_data = snippets_data.values[split]\n","        # print(f'len sub_snippets_data: {len(sub_snippets_data)}')\n","\n","        \n","\n","        tmp = dataset_generator(sub_main_data, sub_snippets_data, label_order)\n","        if isplit == 0:\n","            generator = torch.utils.data.DataLoader(tmp, **params[0])\n","        else:\n","            generator = torch.utils.data.DataLoader(tmp, **params[1])\n","\n","        generators.append(generator)\n","\n","        # print(sub_main_data)\n","        # print(sub_snippets_data)\n","        # print(f'tmp: \\n {tmp[0]}')\n","        # gen0 = next(iter(generator))\n","        # print(f'gen0: \\n {gen0}')\n","\n","\n","    # make class weights\n","    labels = main_data.values[splits[0]][:,2]\n","    labels = np.array([label_order.index(v) for v in labels])\n","\n","\n","    if not other_dataset:\n","        label_weights = torch.tensor(compute_class_weight(\"balanced\", classes=np.arange(len(label_order)), y=labels).astype(np.float32))\n","    else:\n","        label_weights = None\n","\n","    return generators[0], generators[1], generators[2], label_weights\n","\n","# evaluate the f1micro and f1 macro scores\n","def evaluate(generator, model, other_from=None, ignore_snippet=None):\n","    all_labels = []\n","    all_predictions = []\n","\n","    all_claimIDs = []\n","    all_logits = []\n","\n","    for vals in generator:\n","        claimIDs, claims, labels, snippets = vals[0], vals[1], vals[2], vals[3]\n","\n","        if ignore_snippet is not None:\n","            for i in range(len(snippets)):\n","                snippets[i][ignore_snippet] = \"filler\"\n","\n","        all_labels += labels\n","        logits = model(claims, snippets)\n","\n","        predictions = torch.argmax(logits, 1).cpu().numpy()\n","\n","        if other_from == \"pomt\": # other data is pomt, and model is trained on snes\n","            # this case is fine\n","            pass\n","        elif other_from == \"snes\": # other data is snes, and model is trained on pomt\n","            # in this case both \"pants on fire!\" and \"false\" should be considered as false\n","            predictions[predictions == 0] = 1 # 0 is \"pants on fire!\" and 1 is \"false\" for pomt.\n","\n","        all_predictions += predictions.tolist()\n","\n","        all_claimIDs += claimIDs\n","        all_logits += logits.cpu().numpy().tolist()\n","\n","    f1_micro = f1_score(all_labels, all_predictions, average=\"micro\")\n","    f1_macro = f1_score(all_labels, all_predictions, average=\"macro\")\n","\n","    return f1_micro, f1_macro, all_claimIDs, all_logits, all_labels, all_predictions\n","\n","def train_step(optimizer, vals, model, criterion):\n","    optimizer.zero_grad()\n","\n","    claimIDs, claims, labels, snippets = vals[0], vals[1], torch.tensor(vals[2]).to(DEVICE), vals[3]\n","\n","    logits = model(claims, snippets)\n","    loss = criterion(logits, labels)\n","\n","    loss.backward()\n","    optimizer.step()\n","\n","    return loss\n","\n","\n","# get embedding matric according to Glove('840B')\n","def get_embedding_matrix(generators, dataset, min_occurrence=1):\n","    savename = \"preprocessed/\" + dataset + \"_glove.pkl\"\n","    if os.path.exists(savename):\n","        tmp = pickle.load(open(savename, \"rb\"))\n","        glove_embedding_matrix = tmp[0]\n","        word2idx = tmp[1]\n","        idx2word = tmp[2]\n","        return glove_embedding_matrix, word2idx, idx2word\n","\n","    glove_vectors = GloVe('840B')\n","    all_claims = []\n","    all_snippets = []\n","    for gen in generators:\n","        for vals in gen:\n","            claims = vals[1]\n","            claims = [clean_str(v) for v in claims]\n","            snippets = vals[3]\n","            snippets = [clean_str(item) for sublist in snippets for item in sublist]\n","\n","            all_claims += claims\n","            all_snippets += snippets\n","\n","    all_words = [word for v in all_claims+all_snippets for word in v.split(\" \")]\n","    counter = Counter(all_words)\n","    all_words = set(all_words)\n","    all_words = list(set([word for word in all_words if counter[word] > min_occurrence]))\n","    word2idx = {word: i+2 for i, word in enumerate(all_words)} # reserve 0 for potential mask and 1 for unk token\n","    idx2word = {word2idx[key]: key for key in word2idx}\n","\n","    num_words = len(idx2word)\n","\n","    glove_embedding_matrix = np.random.random((num_words+2, 300)) - 0.5\n","    missed = 0\n","    for word in word2idx:\n","        if word in glove_vectors:\n","            glove_embedding_matrix[word2idx[word]] = glove_vectors[word]\n","        else:\n","            missed += 1\n","\n","    pickle.dump([glove_embedding_matrix, word2idx, idx2word], open(savename, \"wb\"))\n","    return glove_embedding_matrix, word2idx, idx2word\n","\n","def train_model(model, criterion, optimizer, train_generator, val_generator, test_generator, args, other_generator, savename):\n","    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n","    params = sum([np.prod(p.size()) for p in model_parameters])\n","    print(\"model parameters\", params)\n","\n","    num_epochs = 0\n","    patience_counter = 0\n","    # patience_max = 1\n","    patience_max = 8\n","\n","    best_f1 = -np.inf\n","    while (True):\n","        train_losses = []\n","\n","        model.train()\n","        for ivals, vals in enumerate(train_generator):\n","            loss = train_step(optimizer, vals, model, criterion)\n","            train_losses.append(loss.item())\n","\n","        num_epochs += 1\n","        print_message(\"TRAIN loss\", np.mean(train_losses), num_epochs)\n","\n","        if num_epochs % args.eval_per_epoch == 0:\n","            model.eval()\n","            with torch.no_grad():\n","                val_f1micro, val_f1macro, val_claimIDs, val_logits, val_labels, val_predictions = evaluate(val_generator, model)\n","                print_message(\"VALIDATION F1micro, F1macro, loss:\", val_f1micro, val_f1macro, len(val_claimIDs))\n","\n","            if val_f1macro > best_f1:\n","                with torch.no_grad():\n","                    test_f1micro, test_f1macro, test_claimIDs, test_logits, test_labels, test_predictions = evaluate(test_generator, model)\n","                    print_message(\"TEST F1micro, F1macro, loss:\", test_f1micro, test_f1macro, len(test_claimIDs))\n","\n","                    other_test_f1micro, other_test_f1macro, other_test_claimIDs, other_test_logits, other_test_labels, other_test_predictions = evaluate(other_generator, model, other_from=\"snes\" if args.dataset == \"pomt\" else \"pomt\")\n","                    print_message(\"OTHER-TEST F1micro, F1macro, loss:\", other_test_f1micro, other_test_f1macro, len(other_test_claimIDs))\n","\n","                    test_remove_top_bottom = []\n","                    test_remove_bottom_top = []\n","                    other_test_remove_top_bottom = []\n","                    other_test_remove_bottom_top = []\n","                    ten = np.arange(10)\n","                    if args.inputtype != \"CLAIM_ONLY\":\n","                        for i in tqdm(range(10)):\n","                            top_is = ten[:(i+1)]\n","                            bottom_is = ten[-(i+1):]\n","                            test_remove_top_bottom.append( evaluate(test_generator, model, ignore_snippet=top_is) )\n","                            test_remove_bottom_top.append( evaluate(test_generator, model, ignore_snippet=bottom_is) )\n","                            other_test_remove_top_bottom.append(evaluate(other_generator, model, other_from=\"snes\" if args.dataset == \"pomt\" else \"pomt\", ignore_snippet=top_is))\n","                            other_test_remove_bottom_top.append(evaluate(other_generator, model, other_from=\"snes\" if args.dataset == \"pomt\" else \"pomt\", ignore_snippet=bottom_is))\n","\n","                        print_message([np.around(v[1], 4) for v in test_remove_top_bottom])\n","                        print_message([np.around(v[1], 4) for v in test_remove_bottom_top])\n","                        print_message([np.around(v[1], 4) for v in other_test_remove_top_bottom])\n","                        print_message([np.around(v[1], 4) for v in other_test_remove_bottom_top])\n","\n","                patience_counter = 0\n","                best_f1 = val_f1macro\n","                val_store = [val_f1micro, val_f1macro, val_claimIDs, val_logits, val_labels, val_predictions]\n","                test_store = [test_f1micro, test_f1macro, test_claimIDs, test_logits, test_labels, test_predictions, test_remove_top_bottom, test_remove_bottom_top]\n","                other_test_store = [other_test_f1micro, other_test_f1macro, other_test_claimIDs, other_test_logits, other_test_labels, other_test_predictions, other_test_remove_top_bottom, other_test_remove_bottom_top]\n","                misc_store = [args]\n","                total_store = [val_store, test_store, other_test_store, misc_store]\n","            else:\n","                patience_counter += 1\n","\n","            print_message(\"PATIENCE\", patience_counter, \"/\", patience_max)\n","\n","            if patience_counter >= patience_max:\n","                pickle.dump(total_store, open(savename, \"wb\"))\n","                break\n","\n","# run the bert model\n","def run_bert(args, train_generator, val_generator, test_generator, label_weights, inputtype, label_order, savename, other_generator, step):\n","    print(f'***run_bert*** with inputtype {args.inputtype}')\n","\n","    if step == 'EMO_INT' or step == 'EMO_LEXI':      \n","      model = emoMyBertModel.from_pretrained(BERT_MODEL_PATH, labelnum=len(label_order), input_type=inputtype, emocred_type = step)\n","    elif step == 'EMO_ATT2_INT' or step == 'EMO_ATT2_LEXI':      \n","      model = emoAtt2MyBertModel.from_pretrained(BERT_MODEL_PATH, labelnum=len(label_order), input_type=inputtype, emocred_type = step)\n","    else:\n","      model = MyBertModel.from_pretrained(BERT_MODEL_PATH, labelnum=len(label_order), input_type=inputtype)\n","    model.to(DEVICE)\n","\n","    criterion = torch.nn.CrossEntropyLoss(weight=label_weights.to(DEVICE))\n","    optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr, eps=1e-8)\n","    optimizer.zero_grad()\n","\n","    train_model(model, criterion, optimizer, train_generator, val_generator, test_generator, args, other_generator, savename)\n","\n","# run lstm model\n","def run_lstm(args, train_generator, val_generator, test_generator, label_weights, inputtype, label_order, savename, other_generator):\n","    print(f'***run_lstm*** with inputtype {args.inputtype}')\n","    glove_embedding_matrix, word2idx, idx2word = get_embedding_matrix([train_generator, val_generator, test_generator, other_generator], args.dataset)\n","\n","    model = LSTMModel(args.lstm_hidden_dim, args.lstm_layers, args.lstm_dropout, len(label_order), word2idx, glove_embedding_matrix, input_type=inputtype)\n","    model.to(DEVICE)\n","\n","    criterion = torch.nn.CrossEntropyLoss(weight=label_weights.to(DEVICE))\n","    optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr, eps=1e-8)\n","    optimizer.zero_grad()\n","\n","    train_model(model, criterion, optimizer, train_generator, val_generator, test_generator, args, other_generator, savename)\n","\n","def filter_snippet_for_bow(generator, ignore_snippet, inputtype):\n","    samples = []\n","    for vals in generator:\n","        claims = vals[1]\n","        labels = vals[2]\n","        snippets = vals[3]\n","\n","        for i in range(len(snippets)):\n","            snippets[i][ignore_snippet] = \"filler\"\n","\n","        for i in range(len(claims)):\n","            if inputtype == CLAIM_AND_EVIDENCE:\n","                sample = clean_str(claims[i]) + \" \".join([clean_str(v) for v in snippets[i]])\n","            elif inputtype == CLAIM_ONLY:\n","                sample = clean_str(claims[i])\n","            elif inputtype == EVIDENCE_ONLY:\n","                sample = \" \".join([clean_str(v) for v in snippets[i]])\n","            else:\n","                raise Exception(\"Unknown type\", inputtype)\n","            samples.append(sample)\n","    return samples\n","\n","def get_bows_labels(generators, dataset, inputtype):\n","    all_samples = []\n","    all_labels = []\n","\n","    for gen in generators:\n","        gen_samples = []\n","        gen_labels = []\n","        for vals in gen:\n","            claims = vals[1]\n","            labels = vals[2]\n","            snippets = vals[3]\n","\n","            for i in range(len(claims)):\n","                if inputtype == CLAIM_AND_EVIDENCE:\n","                    sample = clean_str(claims[i]) + \" \".join([clean_str(v) for v in snippets[i]])\n","                elif inputtype == CLAIM_ONLY:\n","                    sample = clean_str(claims[i])\n","                elif inputtype == EVIDENCE_ONLY:\n","                    sample = \" \".join([clean_str(v) for v in snippets[i]])\n","                else:\n","                    raise Exception(\"Unknown type\", inputtype)\n","                gen_samples.append(sample)\n","                gen_labels.append(labels[i])\n","\n","        all_samples.append(gen_samples)\n","        all_labels.append(gen_labels)\n","\n","    test_remove_top_bottom = []\n","    test_remove_bottom_top = []\n","    other_test_remove_top_bottom = []\n","    other_test_remove_bottom_top = []\n","    ten = np.arange(10)\n","    for i in tqdm(range(10)):\n","        top_is = ten[:(i + 1)]\n","        bottom_is = ten[-(i + 1):]\n","        test_remove_top_bottom.append( filter_snippet_for_bow(generators[-2], top_is, inputtype) )\n","        test_remove_bottom_top.append( filter_snippet_for_bow(generators[-2], bottom_is, inputtype) )\n","        other_test_remove_top_bottom.append( filter_snippet_for_bow(generators[-1], top_is, inputtype) )\n","        other_test_remove_bottom_top.append( filter_snippet_for_bow(generators[-1], bottom_is, inputtype) )\n","\n","    vectorizer = TfidfVectorizer(min_df=2)\n","    vectorizer.fit([item for sublist in all_samples for item in sublist])\n","\n","    bows = [vectorizer.transform(all_samples[i]) for i in range(len(all_samples))]\n","\n","    test_remove_top_bottom = [vectorizer.transform(test_remove_top_bottom[i]) for i in range(len(test_remove_top_bottom))]\n","    test_remove_bottom_top = [vectorizer.transform(test_remove_bottom_top[i]) for i in range(len(test_remove_bottom_top))]\n","    other_test_remove_top_bottom = [vectorizer.transform(other_test_remove_top_bottom[i]) for i in range(len(other_test_remove_top_bottom))]\n","    other_test_remove_bottom_top = [vectorizer.transform(other_test_remove_bottom_top[i]) for i in range(len(other_test_remove_bottom_top))]\n","\n","    return bows, all_labels, test_remove_top_bottom, test_remove_bottom_top, other_test_remove_top_bottom, other_test_remove_bottom_top\n","\n","# random forest model\n","def run_bow(args, train_generator, val_generator, test_generator, label_weights, inputtype, label_order, savename, other_test_generator):\n","    # print(f'train_generator0 :\\n {next(iter(train_generator))}')\n","    print(f'***run_bow*** with inputtype {args.inputtype}')\n","\n","    bows, labels, test_remove_top_bottom, test_remove_bottom_top, other_test_remove_top_bottom, other_test_remove_bottom_top = get_bows_labels([train_generator, val_generator, test_generator, other_test_generator], args.dataset, inputtype)\n","\n","    train_bow, val_bow, test_bow, other_test_bow = bows[0], bows[1], bows[2], bows[3]\n","    train_labels, val_labels, test_labels, other_test_labels = labels[0], labels[1], labels[2], labels[3]\n","\n","    label_weights = label_weights.numpy()\n","    weights = {i: label_weights[i] for i in range(len(label_weights))}\n","\n","    # print(f'****** run bow train_bow \\n {train_bow}')\n","    # print('*********')\n","\n","    param_grid = [\n","        {'n_estimators': [100, 500, 1000], 'min_samples_leaf': [1, 3, 5, 10], 'min_samples_split': [2, 5, 10]}\n","    ]\n","\n","    opt = GridSearch(model=RandomForestClassifier(n_jobs=5, class_weight=weights), param_grid=param_grid, parallelize=False)\n","\n","    \n","    opt.fit(train_bow, train_labels, val_bow, val_labels, scoring=\"f1_macro\")\n","\n","    def rf_eval(model, bow, labels, other_from=None):\n","        preds = model.predict(bow)\n","\n","        if other_from == \"pomt\": # other data is pomt, and model is trained on snes\n","            # this case is fine\n","            pass\n","        elif other_from == \"snes\": # other data is snes, and model is trained on pomt\n","            # in this case both \"pants on fire!\" and \"false\" should be considered as false\n","            preds[preds == 0] = 1 # 0 is \"pants on fire!\" and 1 is \"false\" for pomt.\n","\n","        f1_macro = f1_score(labels, preds, average=\"macro\")\n","        f1_micro = f1_score(labels, preds, average=\"micro\")\n","        return f1_micro, f1_macro, labels, preds\n","\n","    # val_store = [val_f1micro, val_f1macro, val_claimIDs, val_logits, val_labels, val_predictions]\n","    # test_store = [test_f1micro, test_f1macro, test_claimIDs, test_logits, test_labels, test_predictions,test_remove_top_bottom, test_remove_bottom_top]\n","    # other_test_store = [other_test_f1micro, other_test_f1macro, other_test_claimIDs, other_test_logits,\n","    #                     other_test_labels, other_test_predictions, other_test_remove_top_bottom,\n","    #                     other_test_remove_bottom_top]\n","    #misc_store = [args]\n","\n","\n","    val_store = rf_eval(opt, val_bow, val_labels)\n","    test_store = list(rf_eval(opt, test_bow, test_labels)) + [[rf_eval(opt, test_remove_top_bottom[i], test_labels) for i in range(10)],\n","                                                       [rf_eval(opt, test_remove_bottom_top[i], test_labels) for i in range(10)]]\n","    other_test_store = list(rf_eval(opt, other_test_bow, other_test_labels, other_from=\"snes\" if args.dataset == \"pomt\" else \"pomt\")) + [[rf_eval(opt, other_test_remove_top_bottom[i], other_test_labels, other_from=\"snes\" if args.dataset == \"pomt\" else \"pomt\") for i in range(10)],\n","                                                       [rf_eval(opt, other_test_remove_bottom_top[i], other_test_labels, other_from=\"snes\" if args.dataset == \"pomt\" else \"pomt\") for i in range(10)]]\n","    misc_store = [opt.get_best_params()]\n","    total_store = [val_store, test_store, other_test_store, misc_store]\n","\n","    print_message(\"VALIDATION\", val_store[0], val_store[1])\n","    print_message(\"TEST\", test_store[0], test_store[1])\n","    print_message(\"OTHER-TEST\", other_test_store[0], other_test_store[1])\n","\n","    print_message([np.around(v[1], 4) for v in test_store[-2]])\n","    print_message([np.around(v[1], 4) for v in test_store[-1]])\n","    print_message([np.around(v[1], 4) for v in other_test_store[-2]])\n","    print_message([np.around(v[1], 4) for v in other_test_store[-1]])\n","    print(misc_store)\n","\n","    pickle.dump(total_store, open(savename, \"wb\"))\n","\n","def filter_websites(snippets_data):\n","    bad_websites = [\"factcheck.org\", \"politifact.com\", \"snopes.com\", \"fullfact.org\", \"factscan.ca\"]\n","    ids = snippets_data.values[:, 0]\n","    remove_count = 0\n","    for i, id in enumerate(ids):\n","        with open(\"../../multi_fc_publicdata/snippets/\" + id, \"r\", encoding=\"utf-8\") as f:\n","            lines = f.readlines()\n","\n","        links = [line.strip().split(\"\\t\")[-1] for line in lines]\n","        remove = [False for _ in range(10)]\n","        for j in range(len(links)):\n","            remove[j] = any([bad in links[j] for bad in bad_websites])\n","        remove = remove[:10]  # 1 data sample has 11 links by mistake in the dataset\n","        snippets_data.iloc[i, [False] + remove] = \"filler\"\n","\n","        remove_count += np.sum(remove)\n","    print_message(\"REMOVE COUNT\", remove_count)\n","    return snippets_data\n","\n"],"id":"eba4c237","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Error loading SnowballStemmer: Package 'SnowballStemmer'\n","[nltk_data]     not found in index\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"]}]},{"cell_type":"code","metadata":{"id":"5609720e","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["fd7baa5d4de94e1da0e246ca95aa5f24","eeae52394a16483d9960553347906f38","0f045c426fa748d78fe8171781756a7c","19f2a44abc1248828277d55c2e648097","63e4b06bc7244857baf4593562f63847","fc518c5b70724ce9b8c2639ce3bfe858","3d53c1d1114e42ed99f8e08ae63b3249","80a19ec770da4646a1110276ea470da9","78a8631585594898ad1a247b0d8e71ac","bb106d3930bc4b24b461092f2c3072e6","78c6c0841f1c49c7b0cbd409672fb8db","8a3ba8cf6f62435082823a0deb1d2205","f4a9cb4a730e49b496727bba34543454","f9d378b9fd6f45b89490a6400bfa8be6","d509c8761fef45c187d7252233b09760","8587edcae5a64f69b6453bceab072def","49d9fda89fd74b2caea7b3a521097291","579e2cfe2812475892ef90078a4532d8","fcf30eabe15a4a62a0577520bab314bf","82ebb5412d2141999fc4bbc90354c565","ab0cef77bd0d489fbfb274d180cf530a","16d9c1b732c64373a38ca1983179b66f","e33727377d954afda1704674abd7099c","b952ea221b5f48d4a742c587410fd65d","410822e0afb2440a976cb33b4999a0a9","b81d97d9ba89436c9f6d2bf5ea9d8558","17b1ebdbfe8047ba8106606ec9aec2e3","e9faa01a89e348b8aab458979c5363d8","0eb83823ad3c49a0b5e23af64d14649f","5c2e5ff785334bf99bd40b4821adbcca","0a872307d263412faf6d1725f4a32803","4a738bd10a2c479e897036b7f7fd0bc8","bced6864c2394b0bbe3f81b029bd8047","b4dd0c3e1ecb4a7cbd607e076d52fe2d","1258a769bb684938a9834bc1d7a0100a","460ca7dbeb874004934f7b38c1c58e85","933b68ae0a834a5ba9c5fdf92ba8222c","aa7a20bc88134d3eb45f162af74f94b4","0d1e241e3792409baa2e631c2ff7e745","75f42cae2e4d47bba25222fc7bbd403b","48a64a27c24f4ce490a1aa571e5290cf","8cc276d7c7c5479f9e34de2cc4d5f0e1","77fa55c87e3e419eb480950933446a95","cf205de3ac0d4110b1ba19af2afada6c","07dfdb754de644eba178dfc95da313cc","4ab9ac73e2904c27981fdc18f0b83a6c","a21ee861c87f4d4fa6ade51740b05199","40584c8639d34ce7952a95911ad5ab1d","b6c9c44f810349349ae03411718a2943","cb20a449a79d4146b0eb4d3f5f94ab74","eec719933d814600868a07ab8d2b6ef5","9a3b3447fd49489cbac9ae61582a8365","1547526e60b846a09ac39333ab1ab53d","845380d4fe0b4d639fefd698337ac6bb","59cd15361b9849e0bea53fa94cbae161"]},"outputId":"190511b8-678b-423e-efa9-db45b1396877"},"source":["# don't include formal and informal in the steps, need to have separate cell due to we don't have formal and informal data for politifact\n","\n","# the cell will run all the training and evaluation according to the configurations, \n","# and generate the results and store under the results folder\n","import gc\n","\n","gc.collect()\n","class vars():\n","    def __init__(self, mode, inputtype, dataset):\n","        if mode == \"bow\":\n","            self.dataset = dataset\n","            self.inputtype = inputtype\n","            self.filter_websites = 0\n","            self.model = \"bow\"\n","            self.batchsize = 2\n","            self.eval_per_epoch = 1\n","            self.lr = 0.0001\n","        elif mode == 'lstm':\n","            self.dataset = dataset\n","            self.inputtype = inputtype\n","            self.filter_websites = 0\n","            self.model = \"lstm\"\n","            self.batchsize = 16\n","            self.eval_per_epoch = 1\n","            self.lr = 0.0001\n","            self.lstm_hidden_dim = 128\n","            self.lstm_layers = 2\n","            self.lstm_dropout = 0.1\n","        elif mode == 'bert':\n","            self.dataset = dataset\n","            self.inputtype = inputtype\n","            self.filter_websites = 0\n","            self.model = \"bert\"\n","            if self.dataset == \"snes\":\n","              self.batchsize = 6\n","            elif self.dataset == \"pomt\":\n","              self.batchsize = 4\n","            self.eval_per_epoch = 1\n","            self.lr = 0.000003            \n","\n","filepath = 'sorted.uk.word.unigrams'  \n","word_freq = {}  \n","count = 0\n","with open(filepath, encoding= 'utf-8') as f:\n","    for line in f:\n","        line = line.rstrip()\n","        if line:\n","            x = line.split('\\t')\n","            #print(x)\n","            #print(key, val)\n","            #print(str(x[1]))\n","            word_freq[x[1]] = str(x[0])\n","        count +=1\n","        if count > 100000:\n","            break\n","\n","steps = [['none'], ['pos'], ['stop'], ['pos', 'stop'], ['EMO_INT'], ['EMO_ATT2_INT'], ['EMO_LEXI'], ['EMO_ATT2_LEXI'], ['neg'], ['stem'], ['pos', 'neg'], ['pos', 'neg', 'stop'], ['all']]\n","modes = ['bert']\n","datasets = ['snes', 'pomt']\n","inputtypes = ['CLAIM_ONLY', 'CLAIM_AND_EVIDENCE', 'EVIDENCE_ONLY']\n","# inputtypes = ['CLAIM_ONLY']\n","\n","\n","\n","for step in steps:\n","  for mode in modes:    \n","    for dataset in datasets:\n","      for inputtype in inputtypes:       \n","        print('***********************************')\n","        stepstr = \"-\".join([s for s in step])                \n","        args = vars(mode, inputtype, dataset)\n","\n","        if args.filter_websites > 0.5:\n","            savename = \"results/\" + \"-\".join([str(v) for v in [args.filter_websites, args.model, args.dataset, args.inputtype, stepstr, args.lr, args.batchsize]])\n","        else:\n","            savename = \"results/\" + \"-\".join([str(v) for v in [args.model, args.dataset, args.inputtype, stepstr, args.lr, args.batchsize]])\n","\n","        if args.model == \"lstm\":\n","            savename += \"-\" + \"-\".join([str(v) for v in [args.lstm_hidden_dim, args.lstm_layers, stepstr, args.lstm_dropout]])\n","        savename += \".pkl\"\n","        print(args.inputtype, \"-\", args.dataset, \"-\", stepstr)\n","        inputtype = INPUT_TYPE_ORDER.index(args.inputtype)\n","        main_data, snippets_data, label_order, splits = load_data(args.dataset, stepstr)\n","\n","        if args.filter_websites > 0.5:\n","            snippets_data = filter_websites(snippets_data)\n","\n","        params = {\"batch_size\": args.batchsize, \"shuffle\": True, \"num_workers\": 1, \"collate_fn\": transformer_collate, \"persistent_workers\": True, \"prefetch_factor\":5}\n","        eval_params = {\"batch_size\": args.batchsize, \"shuffle\": False, \"num_workers\": 1, \"collate_fn\": transformer_collate, \"persistent_workers\": True, \"prefetch_factor\":5}\n","\n","        train_generator, val_generator, test_generator, label_weights = make_generators(main_data, snippets_data, label_order, splits, [params, eval_params])\n","\n","        if stepstr == 'formal' or stepstr == 'informal':\n","          if args.dataset == \"snes\":\n","              main_data, snippets_data, _, splits = load_data(\"pomt\", 'none')\n","              if args.filter_websites > 0.5:\n","                  snippets_data = filter_websites(snippets_data)\n","              main_data.iloc[main_data.iloc[:, 2] == \"pants on fire!\", 2] = \"false\"\n","              main_data.iloc[main_data.iloc[:, 2] == \"half-true\", 2] = \"mixture\"\n","              _, _, other_test_generator, _ = make_generators(main_data, snippets_data, label_order, splits, [params, eval_params], other_dataset=True)\n","          else:\n","              main_data, snippets_data, _, splits = load_data(\"snes\", 'stepstr')\n","              if args.filter_websites > 0.5:\n","                  snippets_data = filter_websites(snippets_data)\n","              main_data.iloc[main_data.iloc[:, 2] == \"mixture\", 2] = \"half-true\"\n","              _, _, other_test_generator, _ = make_generators(main_data, snippets_data, label_order, splits, [params, eval_params], other_dataset=True)\n","        else:\n","          if args.dataset == \"snes\":\n","              main_data, snippets_data, _, splits = load_data(\"pomt\", stepstr)\n","              if args.filter_websites > 0.5:\n","                  snippets_data = filter_websites(snippets_data)\n","              main_data.iloc[main_data.iloc[:, 2] == \"pants on fire!\", 2] = \"false\"\n","              main_data.iloc[main_data.iloc[:, 2] == \"half-true\", 2] = \"mixture\"\n","              _, _, other_test_generator, _ = make_generators(main_data, snippets_data, label_order, splits, [params, eval_params], other_dataset=True)\n","          else:\n","              main_data, snippets_data, _, splits = load_data(\"snes\", stepstr)\n","              if args.filter_websites > 0.5:\n","                  snippets_data = filter_websites(snippets_data)\n","              main_data.iloc[main_data.iloc[:, 2] == \"mixture\", 2] = \"half-true\"\n","              _, _, other_test_generator, _ = make_generators(main_data, snippets_data, label_order, splits, [params, eval_params], other_dataset=True)\n","\n","\n","        if args.model == \"bert\":\n","            run_bert(args, train_generator, val_generator, test_generator, label_weights, inputtype, label_order, savename, other_test_generator, stepstr)\n","        elif args.model == \"lstm\":\n","            run_lstm(args, train_generator, val_generator, test_generator, label_weights, inputtype, label_order, savename, other_test_generator)\n","        elif args.model == \"bow\":\n","            # print(\"run bow\")\n","            run_bow(args, train_generator, val_generator, test_generator, label_weights, inputtype, label_order, savename, other_test_generator)\n","\n","        gc.collect()\n","\n","\n"],"id":"5609720e","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["***********************************\n","CLAIM_ONLY - snes - none\n","load data function: step = none\n","****load data: none-preprocess, EMO_LEXI, EMO_INT, EMO_ATT2_LEXI, EMO_ATT2_INT *****\n","len 5069 , false (64.3\\%), mostly false (7.5\\%), mixture (12.3\\%), mostly true (2.8\\%), true (13.0\\%)\n","load data function: step = none\n","****load data: none-preprocess, EMO_LEXI, EMO_INT, EMO_ATT2_LEXI, EMO_ATT2_INT *****\n","len 13581 , false (29.7\\%), mostly false (17.0\\%), mixture (19.8\\%), mostly true (18.8\\%), true (14.8\\%)\n","***run_bert*** with inputtype CLAIM_ONLY\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fd7baa5d4de94e1da0e246ca95aa5f24","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8a3ba8cf6f62435082823a0deb1d2205","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e33727377d954afda1704674abd7099c","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b4dd0c3e1ecb4a7cbd607e076d52fe2d","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"07dfdb754de644eba178dfc95da313cc","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing MyBertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing MyBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing MyBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of MyBertModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['predictor.weight', 'predictor.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["model parameters 109486085\n","[Dec 07, 03:53:33] TRAIN loss 1.5681469057862822 1\n","[Dec 07, 03:53:34] VALIDATION F1micro, F1macro, loss: 0.5601577909270217 0.23094769422691983 507\n","[Dec 07, 03:53:37] TEST F1micro, F1macro, loss: 0.5769230769230769 0.25863930753619935 1014\n","[Dec 07, 03:53:43] OTHER-TEST F1micro, F1macro, loss: 0.2664703717335296 0.19651146908710806 2717\n","[Dec 07, 03:53:43] PATIENCE 0 / 1\n","[Dec 07, 03:54:11] TRAIN loss 1.473673742063142 2\n","[Dec 07, 03:54:13] VALIDATION F1micro, F1macro, loss: 0.5108481262327417 0.2493260345520289 507\n","[Dec 07, 03:54:15] TEST F1micro, F1macro, loss: 0.5335305719921104 0.27918475336288706 1014\n","[Dec 07, 03:54:21] OTHER-TEST F1micro, F1macro, loss: 0.25027603974972396 0.19710815085836741 2717\n","[Dec 07, 03:54:21] PATIENCE 0 / 1\n","[Dec 07, 03:54:50] TRAIN loss 1.3792523119497944 3\n","[Dec 07, 03:54:51] VALIDATION F1micro, F1macro, loss: 0.5069033530571992 0.25806533109372226 507\n","[Dec 07, 03:54:53] TEST F1micro, F1macro, loss: 0.5335305719921104 0.29318483947684226 1014\n","[Dec 07, 03:55:00] OTHER-TEST F1micro, F1macro, loss: 0.24696356275303644 0.2002666588969519 2717\n","[Dec 07, 03:55:00] PATIENCE 0 / 1\n","[Dec 07, 03:55:28] TRAIN loss 1.2677433995177616 4\n","[Dec 07, 03:55:30] VALIDATION F1micro, F1macro, loss: 0.4891518737672584 0.27410773092796725 507\n","[Dec 07, 03:55:32] TEST F1micro, F1macro, loss: 0.4970414201183432 0.27879863306908026 1014\n","[Dec 07, 03:55:39] OTHER-TEST F1micro, F1macro, loss: 0.24696356275303644 0.19614879694035786 2717\n","[Dec 07, 03:55:39] PATIENCE 0 / 1\n","[Dec 07, 03:56:07] TRAIN loss 1.1506704904541776 5\n","[Dec 07, 03:56:09] VALIDATION F1micro, F1macro, loss: 0.42800788954635116 0.25622065272023564 507\n","[Dec 07, 03:56:09] PATIENCE 1 / 1\n","***********************************\n","CLAIM_ONLY - pomt - none\n","load data function: step = none\n","****load data: none-preprocess, EMO_LEXI, EMO_INT, EMO_ATT2_LEXI, EMO_ATT2_INT *****\n","len 13581 , pants on fire! (10.6\\%), false (19.2\\%), mostly false (17.0\\%), half-true (19.8\\%), mostly true (18.8\\%), true (14.8\\%)\n","load data function: step = none\n","****load data: none-preprocess, EMO_LEXI, EMO_INT, EMO_ATT2_LEXI, EMO_ATT2_INT *****\n","len 5069 , pants on fire! (0.0\\%), false (64.3\\%), mostly false (7.5\\%), half-true (12.3\\%), mostly true (2.8\\%), true (13.0\\%)\n","***run_bert*** with inputtype CLAIM_ONLY\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing MyBertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing MyBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing MyBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of MyBertModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['predictor.weight', 'predictor.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["model parameters 109486854\n","[Dec 07, 03:58:11] TRAIN loss 1.7330817797879086 1\n","[Dec 07, 03:58:16] VALIDATION F1micro, F1macro, loss: 0.2614138438880707 0.2508111024718848 1358\n","[Dec 07, 03:58:26] TEST F1micro, F1macro, loss: 0.27751196172248804 0.2713815110584175 2717\n","[Dec 07, 03:58:29] OTHER-TEST F1micro, F1macro, loss: 0.6084812623274162 0.18229432043209665 1014\n","[Dec 07, 03:58:29] PATIENCE 0 / 1\n","[Dec 07, 04:00:27] TRAIN loss 1.6386733780551168 2\n","[Dec 07, 04:00:31] VALIDATION F1micro, F1macro, loss: 0.28350515463917525 0.2729415680795741 1358\n","[Dec 07, 04:00:41] TEST F1micro, F1macro, loss: 0.27346337872653664 0.2669269934344943 2717\n","[Dec 07, 04:00:45] OTHER-TEST F1micro, F1macro, loss: 0.5956607495069034 0.2079070772618529 1014\n","[Dec 07, 04:00:45] PATIENCE 0 / 1\n","[Dec 07, 04:02:40] TRAIN loss 1.5477345454803173 3\n","[Dec 07, 04:02:45] VALIDATION F1micro, F1macro, loss: 0.25184094256259204 0.24033149497060557 1358\n","[Dec 07, 04:02:45] PATIENCE 1 / 1\n","***********************************\n","CLAIM_ONLY - snes - pos\n","load data function: step = pos\n","****load data: preprocess *****\n","len 5069 , false (64.3\\%), mostly false (7.5\\%), mixture (12.3\\%), mostly true (2.8\\%), true (13.0\\%)\n","load data function: step = pos\n","****load data: preprocess *****\n","len 13581 , false (29.7\\%), mostly false (17.0\\%), mixture (19.8\\%), mostly true (18.8\\%), true (14.8\\%)\n","***run_bert*** with inputtype CLAIM_ONLY\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing MyBertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing MyBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing MyBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of MyBertModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['predictor.weight', 'predictor.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["model parameters 109486085\n","[Dec 07, 04:11:05] TRAIN loss 1.5598043862830948 1\n","[Dec 07, 04:11:06] VALIDATION F1micro, F1macro, loss: 0.5996055226824457 0.18205483687037835 507\n","[Dec 07, 04:11:09] TEST F1micro, F1macro, loss: 0.6321499013806706 0.24438744413310792 1014\n","[Dec 07, 04:11:15] OTHER-TEST F1micro, F1macro, loss: 0.28781744571218254 0.16672312298304764 2717\n","[Dec 07, 04:11:15] PATIENCE 0 / 1\n","[Dec 07, 04:11:44] TRAIN loss 1.495053425934669 2\n","[Dec 07, 04:11:45] VALIDATION F1micro, F1macro, loss: 0.5069033530571992 0.25697416145945556 507\n","[Dec 07, 04:11:48] TEST F1micro, F1macro, loss: 0.5216962524654832 0.28078498732665413 1014\n","[Dec 07, 04:11:54] OTHER-TEST F1micro, F1macro, loss: 0.26352594773647403 0.2123171553776701 2717\n","[Dec 07, 04:11:54] PATIENCE 0 / 1\n","[Dec 07, 04:12:23] TRAIN loss 1.4040468534505046 3\n","[Dec 07, 04:12:24] VALIDATION F1micro, F1macro, loss: 0.4536489151873767 0.25562334827433053 507\n","[Dec 07, 04:12:24] PATIENCE 1 / 1\n","***********************************\n","CLAIM_ONLY - pomt - pos\n","load data function: step = pos\n","****load data: preprocess *****\n","len 13581 , pants on fire! (10.6\\%), false (19.2\\%), mostly false (17.0\\%), half-true (19.8\\%), mostly true (18.8\\%), true (14.8\\%)\n","load data function: step = pos\n","****load data: preprocess *****\n","len 5069 , pants on fire! (0.0\\%), false (64.3\\%), mostly false (7.5\\%), half-true (12.3\\%), mostly true (2.8\\%), true (13.0\\%)\n","***run_bert*** with inputtype CLAIM_ONLY\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing MyBertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing MyBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing MyBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of MyBertModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['predictor.weight', 'predictor.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["model parameters 109486854\n","[Dec 07, 04:21:58] TRAIN loss 1.7505800942187055 1\n","[Dec 07, 04:22:02] VALIDATION F1micro, F1macro, loss: 0.24374079528718703 0.221657502301243 1358\n","[Dec 07, 04:22:12] TEST F1micro, F1macro, loss: 0.2418108207581892 0.2220886622762399 2717\n","[Dec 07, 04:22:16] OTHER-TEST F1micro, F1macro, loss: 0.5483234714003945 0.21375322677514283 1014\n","[Dec 07, 04:22:16] PATIENCE 0 / 1\n","[Dec 07, 04:24:14] TRAIN loss 1.6681903899342863 2\n","[Dec 07, 04:24:19] VALIDATION F1micro, F1macro, loss: 0.25699558173784975 0.2549158031498172 1358\n","[Dec 07, 04:24:28] TEST F1micro, F1macro, loss: 0.24696356275303644 0.24815182215441153 2717\n","[Dec 07, 04:24:32] OTHER-TEST F1micro, F1macro, loss: 0.571992110453649 0.2072504740848371 1014\n","[Dec 07, 04:24:32] PATIENCE 0 / 1\n","[Dec 07, 04:26:26] TRAIN loss 1.585647068256383 3\n","[Dec 07, 04:26:31] VALIDATION F1micro, F1macro, loss: 0.2503681885125184 0.24973437248445296 1358\n","[Dec 07, 04:26:31] PATIENCE 1 / 1\n","***********************************\n","CLAIM_ONLY - snes - stop\n","load data function: step = stop\n","****load data: preprocess *****\n","len 5069 , false (64.3\\%), mostly false (7.5\\%), mixture (12.3\\%), mostly true (2.8\\%), true (13.0\\%)\n","load data function: step = stop\n","****load data: preprocess *****\n","len 13581 , false (29.7\\%), mostly false (17.0\\%), mixture (19.8\\%), mostly true (18.8\\%), true (14.8\\%)\n","***run_bert*** with inputtype CLAIM_ONLY\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing MyBertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing MyBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing MyBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of MyBertModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['predictor.weight', 'predictor.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["model parameters 109486085\n","[Dec 07, 04:29:33] TRAIN loss 1.5678082234352022 1\n","[Dec 07, 04:29:34] VALIDATION F1micro, F1macro, loss: 0.5976331360946746 0.21203882567843454 507\n","[Dec 07, 04:29:37] TEST F1micro, F1macro, loss: 0.596646942800789 0.2130029665290519 1014\n","[Dec 07, 04:29:43] OTHER-TEST F1micro, F1macro, loss: 0.26978284873021713 0.18038715194604685 2717\n","[Dec 07, 04:29:43] PATIENCE 0 / 1\n","[Dec 07, 04:30:12] TRAIN loss 1.4895049774566211 2\n","[Dec 07, 04:30:13] VALIDATION F1micro, F1macro, loss: 0.5404339250493096 0.26612133284745837 507\n","[Dec 07, 04:30:16] TEST F1micro, F1macro, loss: 0.539447731755424 0.27080162589014434 1014\n","[Dec 07, 04:30:22] OTHER-TEST F1micro, F1macro, loss: 0.25947736474052263 0.17755002340373505 2717\n","[Dec 07, 04:30:22] PATIENCE 0 / 1\n","[Dec 07, 04:30:51] TRAIN loss 1.4190420456029273 3\n","[Dec 07, 04:30:52] VALIDATION F1micro, F1macro, loss: 0.5562130177514792 0.2994470649127433 507\n","[Dec 07, 04:30:54] TEST F1micro, F1macro, loss: 0.5404339250493096 0.27256024913055027 1014\n","[Dec 07, 04:31:01] OTHER-TEST F1micro, F1macro, loss: 0.25726904674273093 0.20168938519781512 2717\n","[Dec 07, 04:31:01] PATIENCE 0 / 1\n","[Dec 07, 04:31:29] TRAIN loss 1.3171737302799482 4\n","[Dec 07, 04:31:30] VALIDATION F1micro, F1macro, loss: 0.4812623274161736 0.288084518633383 507\n","[Dec 07, 04:31:30] PATIENCE 1 / 1\n","***********************************\n","CLAIM_ONLY - pomt - stop\n","load data function: step = stop\n","****load data: preprocess *****\n","len 13581 , pants on fire! (10.6\\%), false (19.2\\%), mostly false (17.0\\%), half-true (19.8\\%), mostly true (18.8\\%), true (14.8\\%)\n","load data function: step = stop\n","****load data: preprocess *****\n","len 5069 , pants on fire! (0.0\\%), false (64.3\\%), mostly false (7.5\\%), half-true (12.3\\%), mostly true (2.8\\%), true (13.0\\%)\n","***run_bert*** with inputtype CLAIM_ONLY\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing MyBertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing MyBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing MyBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of MyBertModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['predictor.weight', 'predictor.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["model parameters 109486854\n","[Dec 07, 04:35:57] TRAIN loss 1.7506222507007794 1\n","[Dec 07, 04:36:02] VALIDATION F1micro, F1macro, loss: 0.2349042709867452 0.18787998577308693 1358\n","[Dec 07, 04:36:12] TEST F1micro, F1macro, loss: 0.2326094957673905 0.19301658961471155 2717\n","[Dec 07, 04:36:15] OTHER-TEST F1micro, F1macro, loss: 0.6193293885601578 0.1946008964876764 1014\n","[Dec 07, 04:36:15] PATIENCE 0 / 1\n","[Dec 07, 04:38:10] TRAIN loss 1.6721279062691183 2\n","[Dec 07, 04:38:15] VALIDATION F1micro, F1macro, loss: 0.25257731958762886 0.24407552850881406 1358\n","[Dec 07, 04:38:25] TEST F1micro, F1macro, loss: 0.24953993375046007 0.24129565206818884 2717\n","[Dec 07, 04:38:28] OTHER-TEST F1micro, F1macro, loss: 0.5808678500986193 0.26596701287924746 1014\n","[Dec 07, 04:38:28] PATIENCE 0 / 1\n","[Dec 07, 04:40:24] TRAIN loss 1.5900822849799265 3\n","[Dec 07, 04:40:28] VALIDATION F1micro, F1macro, loss: 0.25773195876288657 0.2568740451296739 1358\n","[Dec 07, 04:40:38] TEST F1micro, F1macro, loss: 0.25800515274199487 0.2617355732693554 2717\n","[Dec 07, 04:40:42] OTHER-TEST F1micro, F1macro, loss: 0.5660749506903353 0.2592903828197946 1014\n","[Dec 07, 04:40:42] PATIENCE 0 / 1\n","[Dec 07, 04:42:38] TRAIN loss 1.4928926976352968 4\n","[Dec 07, 04:42:43] VALIDATION F1micro, F1macro, loss: 0.24742268041237114 0.2491447116619843 1358\n","[Dec 07, 04:42:43] PATIENCE 1 / 1\n","***********************************\n","CLAIM_ONLY - snes - pos-stop\n","load data function: step = pos-stop\n","****load data: preprocess *****\n","len 5069 , false (64.3\\%), mostly false (7.5\\%), mixture (12.3\\%), mostly true (2.8\\%), true (13.0\\%)\n","load data function: step = pos-stop\n","****load data: preprocess *****\n","len 13581 , false (29.7\\%), mostly false (17.0\\%), mixture (19.8\\%), mostly true (18.8\\%), true (14.8\\%)\n","***run_bert*** with inputtype CLAIM_ONLY\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing MyBertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing MyBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing MyBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of MyBertModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['predictor.weight', 'predictor.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["model parameters 109486085\n","[Dec 07, 04:51:59] TRAIN loss 1.5648276838096413 1\n","[Dec 07, 04:52:00] VALIDATION F1micro, F1macro, loss: 0.5404339250493096 0.19983821292070209 507\n","[Dec 07, 04:52:02] TEST F1micro, F1macro, loss: 0.5601577909270217 0.2126633959697489 1014\n","[Dec 07, 04:52:09] OTHER-TEST F1micro, F1macro, loss: 0.25874125874125875 0.15579075251361557 2717\n","[Dec 07, 04:52:09] PATIENCE 0 / 1\n","[Dec 07, 04:52:38] TRAIN loss 1.497654100426951 2\n","[Dec 07, 04:52:40] VALIDATION F1micro, F1macro, loss: 0.5305719921104537 0.24211002640017668 507\n","[Dec 07, 04:52:42] TEST F1micro, F1macro, loss: 0.5552268244575936 0.2561071254403142 1014\n","[Dec 07, 04:52:49] OTHER-TEST F1micro, F1macro, loss: 0.26058152373941845 0.19871111367952915 2717\n","[Dec 07, 04:52:49] PATIENCE 0 / 1\n","[Dec 07, 04:53:19] TRAIN loss 1.4120776837540638 3\n","[Dec 07, 04:53:20] VALIDATION F1micro, F1macro, loss: 0.44970414201183434 0.25942981830417 507\n","[Dec 07, 04:53:22] TEST F1micro, F1macro, loss: 0.43984220907297833 0.25860109481288973 1014\n","[Dec 07, 04:53:29] OTHER-TEST F1micro, F1macro, loss: 0.24217887375782113 0.19219677398112112 2717\n","[Dec 07, 04:53:29] PATIENCE 0 / 1\n","[Dec 07, 04:53:58] TRAIN loss 1.3276925715240273 4\n","[Dec 07, 04:53:59] VALIDATION F1micro, F1macro, loss: 0.4181459566074951 0.2603061275844242 507\n","[Dec 07, 04:54:02] TEST F1micro, F1macro, loss: 0.4270216962524655 0.26486834301920165 1014\n","[Dec 07, 04:54:09] OTHER-TEST F1micro, F1macro, loss: 0.23776223776223776 0.20885435548855882 2717\n","[Dec 07, 04:54:09] PATIENCE 0 / 1\n","[Dec 07, 04:54:38] TRAIN loss 1.2217338023254194 5\n","[Dec 07, 04:54:39] VALIDATION F1micro, F1macro, loss: 0.5029585798816568 0.2575156149065917 507\n","[Dec 07, 04:54:39] PATIENCE 1 / 1\n","***********************************\n","CLAIM_ONLY - pomt - pos-stop\n","load data function: step = pos-stop\n","****load data: preprocess *****\n","len 13581 , pants on fire! (10.6\\%), false (19.2\\%), mostly false (17.0\\%), half-true (19.8\\%), mostly true (18.8\\%), true (14.8\\%)\n","load data function: step = pos-stop\n","****load data: preprocess *****\n","len 5069 , pants on fire! (0.0\\%), false (64.3\\%), mostly false (7.5\\%), half-true (12.3\\%), mostly true (2.8\\%), true (13.0\\%)\n","***run_bert*** with inputtype CLAIM_ONLY\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing MyBertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing MyBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing MyBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of MyBertModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['predictor.weight', 'predictor.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["model parameters 109486854\n","[Dec 07, 05:05:25] TRAIN loss 1.7543865447792628 1\n","[Dec 07, 05:05:30] VALIDATION F1micro, F1macro, loss: 0.24005891016200295 0.23236329120536212 1358\n","[Dec 07, 05:05:40] TEST F1micro, F1macro, loss: 0.2465955097534045 0.2402163796728879 2717\n","[Dec 07, 05:05:44] OTHER-TEST F1micro, F1macro, loss: 0.5759368836291914 0.27417484573777345 1014\n","[Dec 07, 05:05:44] PATIENCE 0 / 1\n","[Dec 07, 05:07:44] TRAIN loss 1.677351252366677 2\n","[Dec 07, 05:07:49] VALIDATION F1micro, F1macro, loss: 0.2422680412371134 0.2333093555512169 1358\n","[Dec 07, 05:07:59] TEST F1micro, F1macro, loss: 0.25726904674273093 0.25091150791699446 2717\n","[Dec 07, 05:08:02] OTHER-TEST F1micro, F1macro, loss: 0.5552268244575936 0.24202907962759715 1014\n","[Dec 07, 05:08:02] PATIENCE 0 / 1\n","[Dec 07, 05:10:01] TRAIN loss 1.5938830358494618 3\n","[Dec 07, 05:10:06] VALIDATION F1micro, F1macro, loss: 0.24889543446244478 0.23729387927628862 1358\n","[Dec 07, 05:10:16] TEST F1micro, F1macro, loss: 0.2602134707397865 0.2514522019750571 2717\n","[Dec 07, 05:10:19] OTHER-TEST F1micro, F1macro, loss: 0.5818540433925049 0.2589493771390983 1014\n","[Dec 07, 05:10:19] PATIENCE 0 / 1\n","[Dec 07, 05:12:18] TRAIN loss 1.5016900969494478 4\n","[Dec 07, 05:12:23] VALIDATION F1micro, F1macro, loss: 0.24815905743740796 0.23683850040214174 1358\n","[Dec 07, 05:12:23] PATIENCE 1 / 1\n","***********************************\n","CLAIM_ONLY - snes - EMO_INT\n","load data function: step = EMO_INT\n","****load data: none-preprocess, EMO_LEXI, EMO_INT, EMO_ATT2_LEXI, EMO_ATT2_INT *****\n","len 5069 , false (64.3\\%), mostly false (7.5\\%), mixture (12.3\\%), mostly true (2.8\\%), true (13.0\\%)\n","load data function: step = EMO_INT\n","****load data: none-preprocess, EMO_LEXI, EMO_INT, EMO_ATT2_LEXI, EMO_ATT2_INT *****\n","len 13581 , false (29.7\\%), mostly false (17.0\\%), mixture (19.8\\%), mostly true (18.8\\%), true (14.8\\%)\n","***run_bert*** with inputtype CLAIM_ONLY\n","********emoMyBert********\n","emocred_type:  EMO_INT\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing emoMyBertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing emoMyBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing emoMyBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of emoMyBertModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['emo_attn_score.weight', 'emolinear1.weight', 'predictor.bias', 'emo_attn_score.bias', 'predictor.weight', 'emolinear1.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["model parameters 109486206\n","[Dec 07, 05:13:01] TRAIN loss 1.5562731778702221 1\n","[Dec 07, 05:13:02] VALIDATION F1micro, F1macro, loss: 0.5009861932938856 0.22782787007043298 507\n","[Dec 07, 05:13:05] TEST F1micro, F1macro, loss: 0.5364891518737672 0.24448672911787664 1014\n","[Dec 07, 05:13:12] OTHER-TEST F1micro, F1macro, loss: 0.25579683474420317 0.1523074603857269 2717\n","[Dec 07, 05:13:12] PATIENCE 0 / 1\n","[Dec 07, 05:13:43] TRAIN loss 1.4743312214677398 2\n","[Dec 07, 05:13:45] VALIDATION F1micro, F1macro, loss: 0.45956607495069035 0.25085826227674507 507\n","[Dec 07, 05:13:47] TEST F1micro, F1macro, loss: 0.4832347140039448 0.2814454620929837 1014\n","[Dec 07, 05:13:55] OTHER-TEST F1micro, F1macro, loss: 0.23849834376150167 0.17233397606259865 2717\n","[Dec 07, 05:13:55] PATIENCE 0 / 1\n","[Dec 07, 05:14:26] TRAIN loss 1.3739964669016567 3\n","[Dec 07, 05:14:27] VALIDATION F1micro, F1macro, loss: 0.52465483234714 0.2671211375444353 507\n","[Dec 07, 05:14:30] TEST F1micro, F1macro, loss: 0.5453648915187377 0.28677256606163043 1014\n","[Dec 07, 05:14:37] OTHER-TEST F1micro, F1macro, loss: 0.2443871917556128 0.16777715399754517 2717\n","[Dec 07, 05:14:37] PATIENCE 0 / 1\n","[Dec 07, 05:15:08] TRAIN loss 1.2582569944395408 4\n","[Dec 07, 05:15:09] VALIDATION F1micro, F1macro, loss: 0.4714003944773176 0.26803405603068053 507\n","[Dec 07, 05:15:12] TEST F1micro, F1macro, loss: 0.4911242603550296 0.3109673742940736 1014\n","[Dec 07, 05:15:19] OTHER-TEST F1micro, F1macro, loss: 0.2340817077659183 0.16939835046439064 2717\n","[Dec 07, 05:15:19] PATIENCE 0 / 1\n","[Dec 07, 05:15:50] TRAIN loss 1.1463200437358103 5\n","[Dec 07, 05:15:52] VALIDATION F1micro, F1macro, loss: 0.4339250493096647 0.2617067444852453 507\n","[Dec 07, 05:15:52] PATIENCE 1 / 1\n","***********************************\n","CLAIM_ONLY - pomt - EMO_INT\n","load data function: step = EMO_INT\n","****load data: none-preprocess, EMO_LEXI, EMO_INT, EMO_ATT2_LEXI, EMO_ATT2_INT *****\n","len 13581 , pants on fire! (10.6\\%), false (19.2\\%), mostly false (17.0\\%), half-true (19.8\\%), mostly true (18.8\\%), true (14.8\\%)\n","load data function: step = EMO_INT\n","****load data: none-preprocess, EMO_LEXI, EMO_INT, EMO_ATT2_LEXI, EMO_ATT2_INT *****\n","len 5069 , pants on fire! (0.0\\%), false (64.3\\%), mostly false (7.5\\%), half-true (12.3\\%), mostly true (2.8\\%), true (13.0\\%)\n","***run_bert*** with inputtype CLAIM_ONLY\n","********emoMyBert********\n","emocred_type:  EMO_INT\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing emoMyBertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing emoMyBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing emoMyBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of emoMyBertModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['emo_attn_score.weight', 'emolinear1.weight', 'predictor.bias', 'emo_attn_score.bias', 'predictor.weight', 'emolinear1.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["model parameters 109486983\n","[Dec 07, 05:18:03] TRAIN loss 1.735954197906475 1\n","[Dec 07, 05:18:08] VALIDATION F1micro, F1macro, loss: 0.2496318114874816 0.2378294854223146 1358\n","[Dec 07, 05:18:19] TEST F1micro, F1macro, loss: 0.26426205373573797 0.25660214339466747 2717\n","[Dec 07, 05:18:23] OTHER-TEST F1micro, F1macro, loss: 0.6134122287968442 0.18663380653039702 1014\n","[Dec 07, 05:18:23] PATIENCE 0 / 1\n","[Dec 07, 05:20:27] TRAIN loss 1.6298218367477901 2\n","[Dec 07, 05:20:32] VALIDATION F1micro, F1macro, loss: 0.2614138438880707 0.24305602731974854 1358\n","[Dec 07, 05:20:42] TEST F1micro, F1macro, loss: 0.2694147957305852 0.252394785857671 2717\n","[Dec 07, 05:20:46] OTHER-TEST F1micro, F1macro, loss: 0.6015779092702169 0.19249683222591712 1014\n","[Dec 07, 05:20:46] PATIENCE 0 / 1\n","[Dec 07, 05:22:49] TRAIN loss 1.53074683973054 3\n","[Dec 07, 05:22:55] VALIDATION F1micro, F1macro, loss: 0.24594992636229748 0.2329692840838343 1358\n","[Dec 07, 05:22:55] PATIENCE 1 / 1\n","***********************************\n","CLAIM_ONLY - snes - EMO_ATT2_INT\n","load data function: step = EMO_ATT2_INT\n","****load data: none-preprocess, EMO_LEXI, EMO_INT, EMO_ATT2_LEXI, EMO_ATT2_INT *****\n","len 5069 , false (64.3\\%), mostly false (7.5\\%), mixture (12.3\\%), mostly true (2.8\\%), true (13.0\\%)\n","load data function: step = EMO_ATT2_INT\n","****load data: none-preprocess, EMO_LEXI, EMO_INT, EMO_ATT2_LEXI, EMO_ATT2_INT *****\n","len 13581 , false (29.7\\%), mostly false (17.0\\%), mixture (19.8\\%), mostly true (18.8\\%), true (14.8\\%)\n","***run_bert*** with inputtype CLAIM_ONLY\n","*** run emoAtt2MyBertModel *****\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing emoAtt2MyBertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing emoAtt2MyBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing emoAtt2MyBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of emoAtt2MyBertModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['linear_final.bias', 'emo_attn_score.weight', 'emolinear1.weight', 'emo_linear.weight', 'text_linear.bias', 'predictor.bias', 'linear_final.weight', 'text_linear.weight', 'emo_linear.bias', 'emo_attn_score.bias', 'predictor.weight', 'emolinear1.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["model parameters 109536256\n","[Dec 07, 05:23:33] TRAIN loss 1.5528038007182043 1\n","[Dec 07, 05:23:35] VALIDATION F1micro, F1macro, loss: 0.5088757396449705 0.20499074543375592 507\n","[Dec 07, 05:23:38] TEST F1micro, F1macro, loss: 0.5305719921104537 0.21115234298842492 1014\n","[Dec 07, 05:23:45] OTHER-TEST F1micro, F1macro, loss: 0.26610231873389767 0.17003164838470192 2717\n","[Dec 07, 05:23:45] PATIENCE 0 / 1\n","[Dec 07, 05:24:16] TRAIN loss 1.4901197223445855 2\n","[Dec 07, 05:24:17] VALIDATION F1micro, F1macro, loss: 0.5502958579881657 0.24274557905179525 507\n","[Dec 07, 05:24:20] TEST F1micro, F1macro, loss: 0.5641025641025641 0.25909809218641344 1014\n","[Dec 07, 05:24:27] OTHER-TEST F1micro, F1macro, loss: 0.2458594037541406 0.17070675182273048 2717\n","[Dec 07, 05:24:27] PATIENCE 0 / 1\n","[Dec 07, 05:24:59] TRAIN loss 1.4121684045405 3\n","[Dec 07, 05:25:01] VALIDATION F1micro, F1macro, loss: 0.5384615384615384 0.2859887873077907 507\n","[Dec 07, 05:25:03] TEST F1micro, F1macro, loss: 0.5157790927021696 0.2671260856126797 1014\n","[Dec 07, 05:25:10] OTHER-TEST F1micro, F1macro, loss: 0.2550607287449393 0.18270411059322372 2717\n","[Dec 07, 05:25:10] PATIENCE 0 / 1\n","[Dec 07, 05:25:42] TRAIN loss 1.3156402982972764 4\n","[Dec 07, 05:25:43] VALIDATION F1micro, F1macro, loss: 0.5424063116370809 0.28862768591893284 507\n","[Dec 07, 05:25:46] TEST F1micro, F1macro, loss: 0.5463510848126233 0.28825853273896035 1014\n","[Dec 07, 05:25:53] OTHER-TEST F1micro, F1macro, loss: 0.26278984173721015 0.20008429060371266 2717\n","[Dec 07, 05:25:53] PATIENCE 0 / 1\n","[Dec 07, 05:26:25] TRAIN loss 1.1940703619573567 5\n","[Dec 07, 05:26:27] VALIDATION F1micro, F1macro, loss: 0.5305719921104537 0.2769165041646083 507\n","[Dec 07, 05:26:27] PATIENCE 1 / 1\n","***********************************\n","CLAIM_ONLY - pomt - EMO_ATT2_INT\n","load data function: step = EMO_ATT2_INT\n","****load data: none-preprocess, EMO_LEXI, EMO_INT, EMO_ATT2_LEXI, EMO_ATT2_INT *****\n","len 13581 , pants on fire! (10.6\\%), false (19.2\\%), mostly false (17.0\\%), half-true (19.8\\%), mostly true (18.8\\%), true (14.8\\%)\n","load data function: step = EMO_ATT2_INT\n","****load data: none-preprocess, EMO_LEXI, EMO_INT, EMO_ATT2_LEXI, EMO_ATT2_INT *****\n","len 5069 , pants on fire! (0.0\\%), false (64.3\\%), mostly false (7.5\\%), half-true (12.3\\%), mostly true (2.8\\%), true (13.0\\%)\n","***run_bert*** with inputtype CLAIM_ONLY\n","*** run emoAtt2MyBertModel *****\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing emoAtt2MyBertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing emoAtt2MyBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing emoAtt2MyBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of emoAtt2MyBertModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['linear_final.bias', 'emo_attn_score.weight', 'emolinear1.weight', 'emo_linear.weight', 'text_linear.bias', 'predictor.bias', 'linear_final.weight', 'text_linear.weight', 'emo_linear.bias', 'emo_attn_score.bias', 'predictor.weight', 'emolinear1.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["model parameters 109537033\n","[Dec 07, 05:28:42] TRAIN loss 1.7322212113118443 1\n","[Dec 07, 05:28:47] VALIDATION F1micro, F1macro, loss: 0.2606774668630339 0.24517384976309095 1358\n","[Dec 07, 05:28:58] TEST F1micro, F1macro, loss: 0.2646301067353699 0.2526516629856767 2717\n","[Dec 07, 05:29:02] OTHER-TEST F1micro, F1macro, loss: 0.6301775147928994 0.20552437045274963 1014\n","[Dec 07, 05:29:02] PATIENCE 0 / 1\n","[Dec 07, 05:31:07] TRAIN loss 1.6337478637043348 2\n","[Dec 07, 05:31:12] VALIDATION F1micro, F1macro, loss: 0.2665684830633284 0.26477643876891105 1358\n","[Dec 07, 05:31:23] TEST F1micro, F1macro, loss: 0.2822966507177033 0.28127391360538584 2717\n","[Dec 07, 05:31:27] OTHER-TEST F1micro, F1macro, loss: 0.6104536489151874 0.2412558397422501 1014\n","[Dec 07, 05:31:27] PATIENCE 0 / 1\n","[Dec 07, 05:33:32] TRAIN loss 1.5396536389317992 3\n","[Dec 07, 05:33:37] VALIDATION F1micro, F1macro, loss: 0.2650957290132548 0.26334334819149374 1358\n","[Dec 07, 05:33:37] PATIENCE 1 / 1\n","***********************************\n","CLAIM_ONLY - snes - EMO_LEXI\n","load data function: step = EMO_LEXI\n","****load data: none-preprocess, EMO_LEXI, EMO_INT, EMO_ATT2_LEXI, EMO_ATT2_INT *****\n","len 5069 , false (64.3\\%), mostly false (7.5\\%), mixture (12.3\\%), mostly true (2.8\\%), true (13.0\\%)\n","load data function: step = EMO_LEXI\n","****load data: none-preprocess, EMO_LEXI, EMO_INT, EMO_ATT2_LEXI, EMO_ATT2_INT *****\n","len 13581 , false (29.7\\%), mostly false (17.0\\%), mixture (19.8\\%), mostly true (18.8\\%), true (14.8\\%)\n","***run_bert*** with inputtype CLAIM_ONLY\n","********emoMyBert********\n","emocred_type:  EMO_LEXI\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing emoMyBertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing emoMyBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing emoMyBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of emoMyBertModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['emo_attn_score.weight', 'emolinear1.weight', 'predictor.bias', 'emo_attn_score.bias', 'predictor.weight', 'emolinear1.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["model parameters 109486206\n","[Dec 07, 05:34:15] TRAIN loss 1.5681306726827815 1\n","[Dec 07, 05:34:17] VALIDATION F1micro, F1macro, loss: 0.6193293885601578 0.20888802554586566 507\n","[Dec 07, 05:34:20] TEST F1micro, F1macro, loss: 0.616370808678501 0.22620600653511494 1014\n","[Dec 07, 05:34:27] OTHER-TEST F1micro, F1macro, loss: 0.2789841737210158 0.18575187609306837 2717\n","[Dec 07, 05:34:27] PATIENCE 0 / 1\n","[Dec 07, 05:34:58] TRAIN loss 1.4772254526615143 2\n","[Dec 07, 05:34:59] VALIDATION F1micro, F1macro, loss: 0.5779092702169625 0.24556002640922864 507\n","[Dec 07, 05:35:02] TEST F1micro, F1macro, loss: 0.6025641025641025 0.2679260887338045 1014\n","[Dec 07, 05:35:09] OTHER-TEST F1micro, F1macro, loss: 0.27788001472212 0.1925779511055597 2717\n","[Dec 07, 05:35:09] PATIENCE 0 / 1\n","[Dec 07, 05:35:40] TRAIN loss 1.387615668310507 3\n","[Dec 07, 05:35:42] VALIDATION F1micro, F1macro, loss: 0.5424063116370809 0.2666938378802158 507\n","[Dec 07, 05:35:44] TEST F1micro, F1macro, loss: 0.5512820512820513 0.28961492282521994 1014\n","[Dec 07, 05:35:52] OTHER-TEST F1micro, F1macro, loss: 0.24990798675009201 0.2017418435808156 2717\n","[Dec 07, 05:35:52] PATIENCE 0 / 1\n","[Dec 07, 05:36:22] TRAIN loss 1.277844857525181 4\n","[Dec 07, 05:36:24] VALIDATION F1micro, F1macro, loss: 0.4457593688362919 0.2798675127849056 507\n","[Dec 07, 05:36:26] TEST F1micro, F1macro, loss: 0.45759368836291914 0.2869150816160777 1014\n","[Dec 07, 05:36:34] OTHER-TEST F1micro, F1macro, loss: 0.23003312476996687 0.17648750052019668 2717\n","[Dec 07, 05:36:34] PATIENCE 0 / 1\n","[Dec 07, 05:37:05] TRAIN loss 1.1577073689046744 5\n","[Dec 07, 05:37:06] VALIDATION F1micro, F1macro, loss: 0.5364891518737672 0.26445506431552945 507\n","[Dec 07, 05:37:06] PATIENCE 1 / 1\n","***********************************\n","CLAIM_ONLY - pomt - EMO_LEXI\n","load data function: step = EMO_LEXI\n","****load data: none-preprocess, EMO_LEXI, EMO_INT, EMO_ATT2_LEXI, EMO_ATT2_INT *****\n","len 13581 , pants on fire! (10.6\\%), false (19.2\\%), mostly false (17.0\\%), half-true (19.8\\%), mostly true (18.8\\%), true (14.8\\%)\n","load data function: step = EMO_LEXI\n","****load data: none-preprocess, EMO_LEXI, EMO_INT, EMO_ATT2_LEXI, EMO_ATT2_INT *****\n","len 5069 , pants on fire! (0.0\\%), false (64.3\\%), mostly false (7.5\\%), half-true (12.3\\%), mostly true (2.8\\%), true (13.0\\%)\n","***run_bert*** with inputtype CLAIM_ONLY\n","********emoMyBert********\n","emocred_type:  EMO_LEXI\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing emoMyBertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing emoMyBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing emoMyBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of emoMyBertModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['emo_attn_score.weight', 'emolinear1.weight', 'predictor.bias', 'emo_attn_score.bias', 'predictor.weight', 'emolinear1.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["model parameters 109486983\n","[Dec 07, 05:39:20] TRAIN loss 1.731783646869218 1\n","[Dec 07, 05:39:25] VALIDATION F1micro, F1macro, loss: 0.24742268041237114 0.25337776309751153 1358\n","[Dec 07, 05:39:36] TEST F1micro, F1macro, loss: 0.2613176297386824 0.26727529919261767 2717\n","[Dec 07, 05:39:40] OTHER-TEST F1micro, F1macro, loss: 0.5710059171597633 0.2169888630511067 1014\n","[Dec 07, 05:39:40] PATIENCE 0 / 1\n","[Dec 07, 05:41:44] TRAIN loss 1.6296287263497917 2\n","[Dec 07, 05:41:49] VALIDATION F1micro, F1macro, loss: 0.26730486008836524 0.2739017426663816 1358\n","[Dec 07, 05:41:59] TEST F1micro, F1macro, loss: 0.2800883327199117 0.2840816522690947 2717\n","[Dec 07, 05:42:03] OTHER-TEST F1micro, F1macro, loss: 0.5818540433925049 0.2353926844209866 1014\n","[Dec 07, 05:42:03] PATIENCE 0 / 1\n","[Dec 07, 05:44:07] TRAIN loss 1.5388417295237073 3\n","[Dec 07, 05:44:12] VALIDATION F1micro, F1macro, loss: 0.25846833578792344 0.2458337013896573 1358\n","[Dec 07, 05:44:12] PATIENCE 1 / 1\n","***********************************\n","CLAIM_ONLY - snes - EMO_ATT2_LEXI\n","load data function: step = EMO_ATT2_LEXI\n","****load data: none-preprocess, EMO_LEXI, EMO_INT, EMO_ATT2_LEXI, EMO_ATT2_INT *****\n","len 5069 , false (64.3\\%), mostly false (7.5\\%), mixture (12.3\\%), mostly true (2.8\\%), true (13.0\\%)\n","load data function: step = EMO_ATT2_LEXI\n","****load data: none-preprocess, EMO_LEXI, EMO_INT, EMO_ATT2_LEXI, EMO_ATT2_INT *****\n","len 13581 , false (29.7\\%), mostly false (17.0\\%), mixture (19.8\\%), mostly true (18.8\\%), true (14.8\\%)\n","***run_bert*** with inputtype CLAIM_ONLY\n","*** run emoAtt2MyBertModel *****\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing emoAtt2MyBertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing emoAtt2MyBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing emoAtt2MyBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of emoAtt2MyBertModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['linear_final.bias', 'emo_attn_score.weight', 'emolinear1.weight', 'emo_linear.weight', 'text_linear.bias', 'predictor.bias', 'linear_final.weight', 'text_linear.weight', 'emo_linear.bias', 'emo_attn_score.bias', 'predictor.weight', 'emolinear1.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["model parameters 109536256\n","[Dec 07, 05:44:53] TRAIN loss 1.5565488835847057 1\n","[Dec 07, 05:44:54] VALIDATION F1micro, F1macro, loss: 0.6331360946745562 0.1927372526361404 507\n","[Dec 07, 05:44:57] TEST F1micro, F1macro, loss: 0.6370808678500987 0.19862536783934165 1014\n","[Dec 07, 05:45:05] OTHER-TEST F1micro, F1macro, loss: 0.3029076186970924 0.14940574302133564 2717\n","[Dec 07, 05:45:05] PATIENCE 0 / 1\n","[Dec 07, 05:45:37] TRAIN loss 1.4916693179591283 2\n","[Dec 07, 05:45:38] VALIDATION F1micro, F1macro, loss: 0.5581854043392505 0.23524425100688579 507\n","[Dec 07, 05:45:41] TEST F1micro, F1macro, loss: 0.5700197238658777 0.2699269211853924 1014\n","[Dec 07, 05:45:48] OTHER-TEST F1micro, F1macro, loss: 0.27051895472948106 0.16917885915223357 2717\n","[Dec 07, 05:45:48] PATIENCE 0 / 1\n","[Dec 07, 05:46:21] TRAIN loss 1.4206278989645276 3\n","[Dec 07, 05:46:22] VALIDATION F1micro, F1macro, loss: 0.47534516765285995 0.23081535141465953 507\n","[Dec 07, 05:46:22] PATIENCE 1 / 1\n","***********************************\n","CLAIM_ONLY - pomt - EMO_ATT2_LEXI\n","load data function: step = EMO_ATT2_LEXI\n","****load data: none-preprocess, EMO_LEXI, EMO_INT, EMO_ATT2_LEXI, EMO_ATT2_INT *****\n","len 13581 , pants on fire! (10.6\\%), false (19.2\\%), mostly false (17.0\\%), half-true (19.8\\%), mostly true (18.8\\%), true (14.8\\%)\n","load data function: step = EMO_ATT2_LEXI\n","****load data: none-preprocess, EMO_LEXI, EMO_INT, EMO_ATT2_LEXI, EMO_ATT2_INT *****\n","len 5069 , pants on fire! (0.0\\%), false (64.3\\%), mostly false (7.5\\%), half-true (12.3\\%), mostly true (2.8\\%), true (13.0\\%)\n","***run_bert*** with inputtype CLAIM_ONLY\n","*** run emoAtt2MyBertModel *****\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing emoAtt2MyBertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing emoAtt2MyBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing emoAtt2MyBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of emoAtt2MyBertModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['linear_final.bias', 'emo_attn_score.weight', 'emolinear1.weight', 'emo_linear.weight', 'text_linear.bias', 'predictor.bias', 'linear_final.weight', 'text_linear.weight', 'emo_linear.bias', 'emo_attn_score.bias', 'predictor.weight', 'emolinear1.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["model parameters 109537033\n","[Dec 07, 05:48:40] TRAIN loss 1.7244977427722528 1\n","[Dec 07, 05:48:46] VALIDATION F1micro, F1macro, loss: 0.26435935198821797 0.25058267609341595 1358\n","[Dec 07, 05:48:57] TEST F1micro, F1macro, loss: 0.27309532572690465 0.26458705219604156 2717\n","[Dec 07, 05:49:01] OTHER-TEST F1micro, F1macro, loss: 0.6301775147928994 0.20225378764953902 1014\n","[Dec 07, 05:49:01] PATIENCE 0 / 1\n","[Dec 07, 05:51:09] TRAIN loss 1.6291028058574035 2\n","[Dec 07, 05:51:15] VALIDATION F1micro, F1macro, loss: 0.2533136966126657 0.2526994137047699 1358\n","[Dec 07, 05:51:25] TEST F1micro, F1macro, loss: 0.2591093117408907 0.26280606255266525 2717\n","[Dec 07, 05:51:29] OTHER-TEST F1micro, F1macro, loss: 0.6055226824457594 0.23516486724098912 1014\n","[Dec 07, 05:51:29] PATIENCE 0 / 1\n","[Dec 07, 05:53:37] TRAIN loss 1.5369995310871252 3\n","[Dec 07, 05:53:42] VALIDATION F1micro, F1macro, loss: 0.2650957290132548 0.26066774131494813 1358\n","[Dec 07, 05:53:53] TEST F1micro, F1macro, loss: 0.2749355907250644 0.27536420017564694 2717\n","[Dec 07, 05:53:57] OTHER-TEST F1micro, F1macro, loss: 0.6094674556213018 0.24465000084366326 1014\n","[Dec 07, 05:53:57] PATIENCE 0 / 1\n","[Dec 07, 05:56:08] TRAIN loss 1.4220255930031556 4\n","[Dec 07, 05:56:13] VALIDATION F1micro, F1macro, loss: 0.27172312223858613 0.27158421676407757 1358\n","[Dec 07, 05:56:24] TEST F1micro, F1macro, loss: 0.27788001472212 0.28176966488735133 2717\n","[Dec 07, 05:56:28] OTHER-TEST F1micro, F1macro, loss: 0.5502958579881657 0.24633561424163292 1014\n","[Dec 07, 05:56:28] PATIENCE 0 / 1\n","[Dec 07, 05:58:38] TRAIN loss 1.2827408922523524 5\n","[Dec 07, 05:58:43] VALIDATION F1micro, F1macro, loss: 0.26362297496318116 0.26946303793487447 1358\n","[Dec 07, 05:58:43] PATIENCE 1 / 1\n","***********************************\n","CLAIM_ONLY - snes - neg\n","load data function: step = neg\n","****load data: preprocess *****\n","len 5069 , false (64.3\\%), mostly false (7.5\\%), mixture (12.3\\%), mostly true (2.8\\%), true (13.0\\%)\n","load data function: step = neg\n","****load data: preprocess *****\n","len 13581 , false (29.7\\%), mostly false (17.0\\%), mixture (19.8\\%), mostly true (18.8\\%), true (14.8\\%)\n","***run_bert*** with inputtype CLAIM_ONLY\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing MyBertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing MyBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing MyBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of MyBertModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['predictor.weight', 'predictor.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["model parameters 109486085\n","[Dec 07, 06:01:11] TRAIN loss 1.5574878606240492 1\n","[Dec 07, 06:01:12] VALIDATION F1micro, F1macro, loss: 0.5029585798816568 0.24802559828042084 507\n","[Dec 07, 06:01:15] TEST F1micro, F1macro, loss: 0.5216962524654832 0.25533844404771366 1014\n","[Dec 07, 06:01:22] OTHER-TEST F1micro, F1macro, loss: 0.24365108575634892 0.19642290450497657 2717\n","[Dec 07, 06:01:22] PATIENCE 0 / 1\n","[Dec 07, 06:01:53] TRAIN loss 1.4774015847895596 2\n","[Dec 07, 06:01:54] VALIDATION F1micro, F1macro, loss: 0.4556213017751479 0.21250307097823878 507\n","[Dec 07, 06:01:54] PATIENCE 1 / 1\n","***********************************\n","CLAIM_ONLY - pomt - neg\n","load data function: step = neg\n","****load data: preprocess *****\n","len 13581 , pants on fire! (10.6\\%), false (19.2\\%), mostly false (17.0\\%), half-true (19.8\\%), mostly true (18.8\\%), true (14.8\\%)\n","load data function: step = neg\n","****load data: preprocess *****\n","len 5069 , pants on fire! (0.0\\%), false (64.3\\%), mostly false (7.5\\%), half-true (12.3\\%), mostly true (2.8\\%), true (13.0\\%)\n","***run_bert*** with inputtype CLAIM_ONLY\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing MyBertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing MyBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing MyBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of MyBertModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['predictor.weight', 'predictor.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["model parameters 109486854\n","[Dec 07, 06:05:55] TRAIN loss 1.7306844549215186 1\n","[Dec 07, 06:06:00] VALIDATION F1micro, F1macro, loss: 0.24594992636229748 0.2482688516200637 1358\n","[Dec 07, 06:06:11] TEST F1micro, F1macro, loss: 0.2679425837320574 0.2721587439328707 2717\n","[Dec 07, 06:06:15] OTHER-TEST F1micro, F1macro, loss: 0.6025641025641025 0.2496328502199825 1014\n","[Dec 07, 06:06:15] PATIENCE 0 / 1\n","[Dec 07, 06:08:21] TRAIN loss 1.6344201790719761 2\n","[Dec 07, 06:08:26] VALIDATION F1micro, F1macro, loss: 0.25773195876288657 0.2638391321502431 1358\n","[Dec 07, 06:08:36] TEST F1micro, F1macro, loss: 0.270887007729113 0.27811459790588827 2717\n","[Dec 07, 06:08:40] OTHER-TEST F1micro, F1macro, loss: 0.5946745562130178 0.2728000764554726 1014\n","[Dec 07, 06:08:40] PATIENCE 0 / 1\n","[Dec 07, 06:10:42] TRAIN loss 1.535312808620664 3\n","[Dec 07, 06:10:47] VALIDATION F1micro, F1macro, loss: 0.23195876288659795 0.22852590774138481 1358\n","[Dec 07, 06:10:47] PATIENCE 1 / 1\n","***********************************\n","CLAIM_ONLY - snes - stem\n","load data function: step = stem\n","****load data: preprocess *****\n","len 5069 , false (64.3\\%), mostly false (7.5\\%), mixture (12.3\\%), mostly true (2.8\\%), true (13.0\\%)\n","load data function: step = stem\n","****load data: preprocess *****\n","len 13581 , false (29.7\\%), mostly false (17.0\\%), mixture (19.8\\%), mostly true (18.8\\%), true (14.8\\%)\n","***run_bert*** with inputtype CLAIM_ONLY\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing MyBertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing MyBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing MyBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of MyBertModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['predictor.weight', 'predictor.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["model parameters 109486085\n","[Dec 07, 06:14:57] TRAIN loss 1.5616625064128153 1\n","[Dec 07, 06:14:59] VALIDATION F1micro, F1macro, loss: 0.5266272189349113 0.21374930304326964 507\n","[Dec 07, 06:15:01] TEST F1micro, F1macro, loss: 0.5443786982248521 0.22592877473634593 1014\n","[Dec 07, 06:15:08] OTHER-TEST F1micro, F1macro, loss: 0.2602134707397865 0.15994827610981216 2717\n","[Dec 07, 06:15:08] PATIENCE 0 / 1\n","[Dec 07, 06:15:39] TRAIN loss 1.5099589542762653 2\n","[Dec 07, 06:15:41] VALIDATION F1micro, F1macro, loss: 0.5581854043392505 0.25861395789629754 507\n","[Dec 07, 06:15:43] TEST F1micro, F1macro, loss: 0.5611439842209073 0.2669040471583006 1014\n","[Dec 07, 06:15:50] OTHER-TEST F1micro, F1macro, loss: 0.24880382775119617 0.1988867741977735 2717\n","[Dec 07, 06:15:50] PATIENCE 0 / 1\n","[Dec 07, 06:16:21] TRAIN loss 1.4229062467410758 3\n","[Dec 07, 06:16:23] VALIDATION F1micro, F1macro, loss: 0.5818540433925049 0.2909908782485241 507\n","[Dec 07, 06:16:25] TEST F1micro, F1macro, loss: 0.5650887573964497 0.270904018433043 1014\n","[Dec 07, 06:16:32] OTHER-TEST F1micro, F1macro, loss: 0.27125506072874495 0.1941305301081617 2717\n","[Dec 07, 06:16:33] PATIENCE 0 / 1\n","[Dec 07, 06:17:04] TRAIN loss 1.3108261560870182 4\n","[Dec 07, 06:17:05] VALIDATION F1micro, F1macro, loss: 0.4930966469428008 0.25999133877297853 507\n","[Dec 07, 06:17:05] PATIENCE 1 / 1\n","***********************************\n","CLAIM_ONLY - pomt - stem\n","load data function: step = stem\n","****load data: preprocess *****\n","len 13581 , pants on fire! (10.6\\%), false (19.2\\%), mostly false (17.0\\%), half-true (19.8\\%), mostly true (18.8\\%), true (14.8\\%)\n","load data function: step = stem\n","****load data: preprocess *****\n","len 5069 , pants on fire! (0.0\\%), false (64.3\\%), mostly false (7.5\\%), half-true (12.3\\%), mostly true (2.8\\%), true (13.0\\%)\n","***run_bert*** with inputtype CLAIM_ONLY\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing MyBertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing MyBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing MyBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of MyBertModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['predictor.weight', 'predictor.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["model parameters 109486854\n","[Dec 07, 06:22:42] TRAIN loss 1.7527585546660434 1\n","[Dec 07, 06:22:47] VALIDATION F1micro, F1macro, loss: 0.23932253313696614 0.23741218098793568 1358\n","[Dec 07, 06:22:58] TEST F1micro, F1macro, loss: 0.23923444976076555 0.23615350059872983 2717\n","[Dec 07, 06:23:02] OTHER-TEST F1micro, F1macro, loss: 0.5867850098619329 0.21139776231979565 1014\n","[Dec 07, 06:23:02] PATIENCE 0 / 1\n","[Dec 07, 06:25:06] TRAIN loss 1.65608539045183 2\n","[Dec 07, 06:25:11] VALIDATION F1micro, F1macro, loss: 0.2496318114874816 0.2472208380103199 1358\n","[Dec 07, 06:25:22] TEST F1micro, F1macro, loss: 0.24806772175193229 0.2432515429277964 2717\n","[Dec 07, 06:25:26] OTHER-TEST F1micro, F1macro, loss: 0.5946745562130178 0.20551429737418947 1014\n","[Dec 07, 06:25:26] PATIENCE 0 / 1\n","[Dec 07, 06:27:31] TRAIN loss 1.5557329946824068 3\n","[Dec 07, 06:27:36] VALIDATION F1micro, F1macro, loss: 0.25773195876288657 0.25388749692355383 1358\n","[Dec 07, 06:27:47] TEST F1micro, F1macro, loss: 0.2539565697460434 0.25376173033998134 2717\n","[Dec 07, 06:27:50] OTHER-TEST F1micro, F1macro, loss: 0.5848126232741617 0.21068134017272952 1014\n","[Dec 07, 06:27:50] PATIENCE 0 / 1\n","[Dec 07, 06:29:52] TRAIN loss 1.436351476620364 4\n","[Dec 07, 06:29:57] VALIDATION F1micro, F1macro, loss: 0.2614138438880707 0.2586022247109666 1358\n","[Dec 07, 06:30:08] TEST F1micro, F1macro, loss: 0.26426205373573797 0.2655093173449846 2717\n","[Dec 07, 06:30:12] OTHER-TEST F1micro, F1macro, loss: 0.5838264299802761 0.23603818212736813 1014\n","[Dec 07, 06:30:12] PATIENCE 0 / 1\n","[Dec 07, 06:32:14] TRAIN loss 1.2919225285584355 5\n","[Dec 07, 06:32:19] VALIDATION F1micro, F1macro, loss: 0.2599410898379971 0.26233376100921013 1358\n","[Dec 07, 06:32:30] TEST F1micro, F1macro, loss: 0.2716231137283769 0.27619516458565496 2717\n","[Dec 07, 06:32:34] OTHER-TEST F1micro, F1macro, loss: 0.5700197238658777 0.2330485231875207 1014\n","[Dec 07, 06:32:34] PATIENCE 0 / 1\n","[Dec 07, 06:34:38] TRAIN loss 1.1599817024994459 6\n","[Dec 07, 06:34:43] VALIDATION F1micro, F1macro, loss: 0.2503681885125184 0.24984774869506854 1358\n","[Dec 07, 06:34:43] PATIENCE 1 / 1\n","***********************************\n","CLAIM_ONLY - snes - pos-neg\n","load data function: step = pos-neg\n","****load data: preprocess *****\n","len 5069 , false (64.3\\%), mostly false (7.5\\%), mixture (12.3\\%), mostly true (2.8\\%), true (13.0\\%)\n","load data function: step = pos-neg\n","****load data: preprocess *****\n","len 13581 , false (29.7\\%), mostly false (17.0\\%), mixture (19.8\\%), mostly true (18.8\\%), true (14.8\\%)\n","***run_bert*** with inputtype CLAIM_ONLY\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing MyBertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing MyBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing MyBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of MyBertModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['predictor.weight', 'predictor.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["model parameters 109486085\n","[Dec 07, 06:43:39] TRAIN loss 1.569862731707257 1\n","[Dec 07, 06:43:40] VALIDATION F1micro, F1macro, loss: 0.46942800788954636 0.2189597148317089 507\n","[Dec 07, 06:43:43] TEST F1micro, F1macro, loss: 0.5177514792899408 0.26229640050494096 1014\n","[Dec 07, 06:43:50] OTHER-TEST F1micro, F1macro, loss: 0.25653294074346705 0.1856654523506282 2717\n","[Dec 07, 06:43:50] PATIENCE 0 / 1\n","[Dec 07, 06:44:20] TRAIN loss 1.4994780793786049 2\n","[Dec 07, 06:44:21] VALIDATION F1micro, F1macro, loss: 0.4792899408284023 0.21896659670119437 507\n","[Dec 07, 06:44:24] TEST F1micro, F1macro, loss: 0.5019723865877712 0.2518601883122589 1014\n","[Dec 07, 06:44:31] OTHER-TEST F1micro, F1macro, loss: 0.2491718807508281 0.15743322663909357 2717\n","[Dec 07, 06:44:31] PATIENCE 0 / 1\n","[Dec 07, 06:45:02] TRAIN loss 1.4296688549220562 3\n","[Dec 07, 06:45:03] VALIDATION F1micro, F1macro, loss: 0.4970414201183432 0.2429788097409506 507\n","[Dec 07, 06:45:05] TEST F1micro, F1macro, loss: 0.5256410256410257 0.27106492834741863 1014\n","[Dec 07, 06:45:12] OTHER-TEST F1micro, F1macro, loss: 0.26610231873389767 0.20740177947128294 2717\n","[Dec 07, 06:45:12] PATIENCE 0 / 1\n","[Dec 07, 06:45:43] TRAIN loss 1.3333593354233213 4\n","[Dec 07, 06:45:44] VALIDATION F1micro, F1macro, loss: 0.4477317554240631 0.24690679160240947 507\n"]}]},{"cell_type":"code","metadata":{"id":"K_079ajyXwUY","executionInfo":{"status":"aborted","timestamp":1638849054099,"user_tz":-480,"elapsed":6,"user":{"displayName":"Wee Yi Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16336291919603696387"}}},"source":["# this cell is for formal and informal in the steps, need to have separate cell due to we don't have formal and informal data for politifact\n","# the cell will run all the training and evaluation according to the configurations, \n","# and generate the results and store under the results folder\n","\n","import gc\n","\n","gc.collect()\n","class vars():\n","    def __init__(self, mode, inputtype, dataset):\n","        if mode == \"bow\":\n","            self.dataset = dataset\n","            self.inputtype = inputtype\n","            self.filter_websites = 0\n","            self.model = \"bow\"\n","            self.batchsize = 2\n","            self.eval_per_epoch = 1\n","            self.lr = 0.0001\n","        elif mode == 'lstm':\n","            self.dataset = dataset\n","            self.inputtype = inputtype\n","            self.filter_websites = 0\n","            self.model = \"lstm\"\n","            self.batchsize = 16\n","            self.eval_per_epoch = 1\n","            self.lr = 0.0001\n","            self.lstm_hidden_dim = 128\n","            self.lstm_layers = 2\n","            self.lstm_dropout = 0.1\n","        elif mode == 'bert':\n","            self.dataset = dataset\n","            self.inputtype = inputtype\n","            self.filter_websites = 0\n","            self.model = \"bert\"\n","            if self.dataset == \"snes\":\n","              self.batchsize = 6\n","            elif self.dataset == \"pomt\":\n","              self.batchsize = 4\n","            self.eval_per_epoch = 1\n","            self.lr = 0.000003            \n","\n","filepath = 'sorted.uk.word.unigrams'  \n","word_freq = {}  \n","count = 0\n","with open(filepath, encoding= 'utf-8') as f:\n","    for line in f:\n","        line = line.rstrip()\n","        if line:\n","            x = line.split('\\t')\n","            #print(x)\n","            #print(key, val)\n","            #print(str(x[1]))\n","            word_freq[x[1]] = str(x[0])\n","        count +=1\n","        if count > 100000:\n","            break\n","\n","# only for formal and informal in the steps, need to have separate cell due to we don't have formal and informal data for politifact\n","steps = [['formal'], ['informal']]\n","modes = ['bert']\n","datasets = ['snes']\n","inputtypes = ['CLAIM_AND_EVIDENCE', 'EVIDENCE_ONLY', 'CLAIM_ONLY']\n","\n","\n","for step in steps:\n","  for mode in modes:    \n","    for dataset in datasets:\n","      for inputtype in inputtypes:       \n","        print('***********************************')\n","        stepstr = \"-\".join([s for s in step])                \n","        args = vars(mode, inputtype, dataset)\n","\n","        if args.filter_websites > 0.5:\n","            savename = \"results/\" + \"-\".join([str(v) for v in [args.filter_websites, args.model, args.dataset, args.inputtype, stepstr, args.lr, args.batchsize]])\n","        else:\n","            savename = \"results/\" + \"-\".join([str(v) for v in [args.model, args.dataset, args.inputtype, stepstr, args.lr, args.batchsize]])\n","\n","        if args.model == \"lstm\":\n","            savename += \"-\" + \"-\".join([str(v) for v in [args.lstm_hidden_dim, args.lstm_layers, stepstr, args.lstm_dropout]])\n","        savename += \".pkl\"\n","        print(args.inputtype, \"-\", args.dataset, \"-\", stepstr)\n","        inputtype = INPUT_TYPE_ORDER.index(args.inputtype)\n","        main_data, snippets_data, label_order, splits = load_data(args.dataset, stepstr)\n","\n","        if args.filter_websites > 0.5:\n","            snippets_data = filter_websites(snippets_data)\n","\n","        params = {\"batch_size\": args.batchsize, \"shuffle\": True, \"num_workers\": 1, \"collate_fn\": transformer_collate, \"persistent_workers\": True, \"prefetch_factor\":5}\n","        eval_params = {\"batch_size\": args.batchsize, \"shuffle\": False, \"num_workers\": 1, \"collate_fn\": transformer_collate, \"persistent_workers\": True, \"prefetch_factor\":5}\n","\n","        train_generator, val_generator, test_generator, label_weights = make_generators(main_data, snippets_data, label_order, splits, [params, eval_params])\n","\n","        if stepstr == 'formal' or stepstr == 'informal':\n","          if args.dataset == \"snes\":\n","              main_data, snippets_data, _, splits = load_data(\"pomt\", 'none')\n","              if args.filter_websites > 0.5:\n","                  snippets_data = filter_websites(snippets_data)\n","              main_data.iloc[main_data.iloc[:, 2] == \"pants on fire!\", 2] = \"false\"\n","              main_data.iloc[main_data.iloc[:, 2] == \"half-true\", 2] = \"mixture\"\n","              _, _, other_test_generator, _ = make_generators(main_data, snippets_data, label_order, splits, [params, eval_params], other_dataset=True)\n","          else:\n","              main_data, snippets_data, _, splits = load_data(\"snes\", 'stepstr')\n","              if args.filter_websites > 0.5:\n","                  snippets_data = filter_websites(snippets_data)\n","              main_data.iloc[main_data.iloc[:, 2] == \"mixture\", 2] = \"half-true\"\n","              _, _, other_test_generator, _ = make_generators(main_data, snippets_data, label_order, splits, [params, eval_params], other_dataset=True)\n","        else:\n","          if args.dataset == \"snes\":\n","              main_data, snippets_data, _, splits = load_data(\"pomt\", stepstr)\n","              if args.filter_websites > 0.5:\n","                  snippets_data = filter_websites(snippets_data)\n","              main_data.iloc[main_data.iloc[:, 2] == \"pants on fire!\", 2] = \"false\"\n","              main_data.iloc[main_data.iloc[:, 2] == \"half-true\", 2] = \"mixture\"\n","              _, _, other_test_generator, _ = make_generators(main_data, snippets_data, label_order, splits, [params, eval_params], other_dataset=True)\n","          else:\n","              main_data, snippets_data, _, splits = load_data(\"snes\", stepstr)\n","              if args.filter_websites > 0.5:\n","                  snippets_data = filter_websites(snippets_data)\n","              main_data.iloc[main_data.iloc[:, 2] == \"mixture\", 2] = \"half-true\"\n","              _, _, other_test_generator, _ = make_generators(main_data, snippets_data, label_order, splits, [params, eval_params], other_dataset=True)\n","\n","\n","        if args.model == \"bert\":\n","            run_bert(args, train_generator, val_generator, test_generator, label_weights, inputtype, label_order, savename, other_test_generator, stepstr)\n","        elif args.model == \"lstm\":\n","            run_lstm(args, train_generator, val_generator, test_generator, label_weights, inputtype, label_order, savename, other_test_generator)\n","        elif args.model == \"bow\":\n","            # print(\"run bow\")\n","            run_bow(args, train_generator, val_generator, test_generator, label_weights, inputtype, label_order, savename, other_test_generator)\n","\n","        gc.collect()\n","\n","\n"],"id":"K_079ajyXwUY","execution_count":null,"outputs":[]}]}